{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorly.decomposition import CP,tucker, parafac, non_negative_tucker\n",
    "from datetime import timedelta, date\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR = pd.read_excel('FINANCIAL_RATIOS_NEW.xlsx', index_col =[0], skiprows=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = None\n",
    "FR_dic = {}\n",
    "FR_list = ['PE', 'PX_LAST', 'PS','PB']\n",
    "n = 0 \n",
    "NUMBER_OF_STOCKS = 220\n",
    "for i,name in enumerate(FR_list) : \n",
    "    FR_dic[name] = (df_FR.iloc[:,n:n+NUMBER_OF_STOCKS]\n",
    "                    .set_index(df_FR.index)\n",
    "                    )\n",
    "\n",
    "    if name not in ['PX_LAST', 'MCAP'] : \n",
    "        # FR_dic[name] = (FR_dic[name] - FR_dic[name].mean()) / FR_dic[name].std()\n",
    "        FR_dic[name] = FR_dic[name].diff()\n",
    "    if name in ['PX_LAST'] :\n",
    "\n",
    "        price = FR_dic['PX_LAST']\n",
    "        FR_dic[name] = np.log(FR_dic[name])\n",
    "        FR_dic[name] = FR_dic[name].diff()\n",
    "    \n",
    "\n",
    "    \n",
    "    FR_dic[name] = FR_dic[name][1:]\n",
    "    n += NUMBER_OF_STOCKS + 2\n",
    "\n",
    "COLUMN_NAMES = FR_dic['PE'].columns\n",
    "FR_index = FR_dic['PE'].index\n",
    "\n",
    "for key, val in FR_dic.items() : \n",
    "    FR_dic[key] = FR_dic[key][1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMAT US Equity</th>\n",
       "      <th>COKE US Equity</th>\n",
       "      <th>WDFC US Equity</th>\n",
       "      <th>AAPL US Equity</th>\n",
       "      <th>KLAC US Equity</th>\n",
       "      <th>SEIC US Equity</th>\n",
       "      <th>CSPI US Equity</th>\n",
       "      <th>ALOT US Equity</th>\n",
       "      <th>AMGN US Equity</th>\n",
       "      <th>CAMP US Equity</th>\n",
       "      <th>...</th>\n",
       "      <th>DVN US Equity</th>\n",
       "      <th>ORCL US Equity</th>\n",
       "      <th>PG US Equity</th>\n",
       "      <th>INTC US Equity</th>\n",
       "      <th>TTMI US Equity</th>\n",
       "      <th>TTEK US Equity</th>\n",
       "      <th>WFC US Equity</th>\n",
       "      <th>WMT US Equity</th>\n",
       "      <th>C US Equity</th>\n",
       "      <th>BA US Equity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-11-24</th>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-1.113</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-01</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>-2.23</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.0200</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>6.93</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>-15.10</td>\n",
       "      <td>-1.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-08</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.2900</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-1.240</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>8.50</td>\n",
       "      <td>-1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-15</th>\n",
       "      <td>-1.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.1700</td>\n",
       "      <td>-0.2687</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>4.80</td>\n",
       "      <td>-5.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-22</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.97</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-2.70</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.0800</td>\n",
       "      <td>-0.2213</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.95</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>1.93</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.21</td>\n",
       "      <td>34.90</td>\n",
       "      <td>2.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-29</th>\n",
       "      <td>-5.84</td>\n",
       "      <td>-19.25</td>\n",
       "      <td>4.29</td>\n",
       "      <td>-6.930</td>\n",
       "      <td>-16.33</td>\n",
       "      <td>-2.220</td>\n",
       "      <td>-0.8490</td>\n",
       "      <td>-0.6273</td>\n",
       "      <td>7.94</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-2.70</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-16.94</td>\n",
       "      <td>-3.15</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>-32.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-06</th>\n",
       "      <td>23.10</td>\n",
       "      <td>150.57</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>21.930</td>\n",
       "      <td>76.47</td>\n",
       "      <td>4.630</td>\n",
       "      <td>1.2790</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>-11.03</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>9.97</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.900</td>\n",
       "      <td>1.09</td>\n",
       "      <td>9.64</td>\n",
       "      <td>5.59</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>6.36</td>\n",
       "      <td>25.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-13</th>\n",
       "      <td>-20.21</td>\n",
       "      <td>-45.76</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-26.010</td>\n",
       "      <td>-40.04</td>\n",
       "      <td>-5.260</td>\n",
       "      <td>-0.1600</td>\n",
       "      <td>-1.4400</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-9.99</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-7.930</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-29.13</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-8.11</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-28.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>11.98</td>\n",
       "      <td>9.85</td>\n",
       "      <td>6.22</td>\n",
       "      <td>-2.070</td>\n",
       "      <td>28.73</td>\n",
       "      <td>2.210</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-28.49</td>\n",
       "      <td>5.27</td>\n",
       "      <td>15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-27</th>\n",
       "      <td>-4.67</td>\n",
       "      <td>73.01</td>\n",
       "      <td>13.93</td>\n",
       "      <td>3.060</td>\n",
       "      <td>-7.23</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>0.9088</td>\n",
       "      <td>-0.2600</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1.44</td>\n",
       "      <td>...</td>\n",
       "      <td>8.50</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>2.53</td>\n",
       "      <td>-13.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AMAT US Equity  COKE US Equity  WDFC US Equity  AAPL US Equity  \\\n",
       "2006-11-24           -1.17           -0.47           -2.35           0.074   \n",
       "2006-12-01           -0.17           -2.23           -0.02           0.016   \n",
       "2006-12-08            0.16            1.40            0.74          -0.112   \n",
       "2006-12-15           -1.19            1.75            0.29           0.009   \n",
       "2006-12-22           -0.16           -0.09            0.97          -0.076   \n",
       "...                    ...             ...             ...             ...   \n",
       "2022-04-29           -5.84          -19.25            4.29          -6.930   \n",
       "2022-05-06           23.10          150.57           -1.76          21.930   \n",
       "2022-05-13          -20.21          -45.76           -5.76         -26.010   \n",
       "2022-05-20           11.98            9.85            6.22          -2.070   \n",
       "2022-05-27           -4.67           73.01           13.93           3.060   \n",
       "\n",
       "            KLAC US Equity  SEIC US Equity  CSPI US Equity  ALOT US Equity  \\\n",
       "2006-11-24           -1.42          -0.925         -0.1300          0.4160   \n",
       "2006-12-01            0.30          -0.440         -0.0200          0.0700   \n",
       "2006-12-08            0.79          -0.445         -0.2900          0.1900   \n",
       "2006-12-15           -2.66          -0.955         -0.1700         -0.2687   \n",
       "2006-12-22           -2.70          -0.565         -0.0800         -0.2213   \n",
       "...                    ...             ...             ...             ...   \n",
       "2022-04-29          -16.33          -2.220         -0.8490         -0.6273   \n",
       "2022-05-06           76.47           4.630          1.2790          0.8873   \n",
       "2022-05-13          -40.04          -5.260         -0.1600         -1.4400   \n",
       "2022-05-20           28.73           2.210          0.0761          0.6100   \n",
       "2022-05-27           -7.23          -1.290          0.9088         -0.2600   \n",
       "\n",
       "            AMGN US Equity  CAMP US Equity  ...  DVN US Equity  \\\n",
       "2006-11-24           -1.59           -0.20  ...           2.26   \n",
       "2006-12-01           -2.09            0.22  ...           6.93   \n",
       "2006-12-08           -3.69            0.29  ...           0.27   \n",
       "2006-12-15           -0.88           -0.66  ...          -1.10   \n",
       "2006-12-22           -0.89            0.98  ...          -3.95   \n",
       "...                    ...             ...  ...            ...   \n",
       "2022-04-29            7.94           -0.92  ...          -8.27   \n",
       "2022-05-06          -11.03            0.53  ...           9.97   \n",
       "2022-05-13           -3.05           -1.26  ...          -8.16   \n",
       "2022-05-20            0.45            0.40  ...           4.95   \n",
       "2022-05-27            8.84            1.44  ...           8.50   \n",
       "\n",
       "            ORCL US Equity  PG US Equity  INTC US Equity  TTMI US Equity  \\\n",
       "2006-11-24           -0.23          1.40          -1.113           -0.84   \n",
       "2006-12-01           -2.05         -1.50          -0.150            0.15   \n",
       "2006-12-08           -1.27         -0.51          -1.240            2.06   \n",
       "2006-12-15           -0.10         -1.01          -0.630           -1.37   \n",
       "2006-12-22           -1.33          1.93          -0.170           -0.34   \n",
       "...                    ...           ...             ...             ...   \n",
       "2022-04-29           -1.27         -2.70          -1.210            1.13   \n",
       "2022-05-06            7.17          0.05           4.900            1.09   \n",
       "2022-05-13           -9.99         -2.58          -7.930           -1.20   \n",
       "2022-05-20           -0.55         -2.55           0.760            0.86   \n",
       "2022-05-27           -2.33         -0.82           0.850           -1.57   \n",
       "\n",
       "            TTEK US Equity  WFC US Equity  WMT US Equity  C US Equity  \\\n",
       "2006-11-24           -1.29          -1.16          -2.33         1.30   \n",
       "2006-12-01           -0.48          -0.87          -1.93       -15.10   \n",
       "2006-12-08           -0.34           0.18          -0.98         8.50   \n",
       "2006-12-15            0.23          -0.62          -0.69         4.80   \n",
       "2006-12-22           -0.58           0.94           1.21        34.90   \n",
       "...                    ...            ...            ...          ...   \n",
       "2022-04-29          -16.94          -3.15          -1.23        -2.61   \n",
       "2022-05-06            9.64           5.59          -1.14         6.36   \n",
       "2022-05-13          -29.13          -1.68          -8.11        -0.91   \n",
       "2022-05-20            0.44           2.60         -28.49         5.27   \n",
       "2022-05-27           -1.80          -0.43          -4.95         2.53   \n",
       "\n",
       "            BA US Equity  \n",
       "2006-11-24          1.14  \n",
       "2006-12-01         -1.08  \n",
       "2006-12-08         -1.28  \n",
       "2006-12-15         -5.78  \n",
       "2006-12-22          2.82  \n",
       "...                  ...  \n",
       "2022-04-29        -32.16  \n",
       "2022-05-06         25.02  \n",
       "2022-05-13        -28.97  \n",
       "2022-05-20         15.38  \n",
       "2022-05-27        -13.89  \n",
       "\n",
       "[810 rows x 220 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TI = pd.read_excel('TECH_NEW.xlsx', skiprows=[0])\n",
    "\n",
    "TI_dic = {}\n",
    "TI_list = ['BB_PERCENT', 'HURST', 'MOM', 'ROC','RSI','WLPR']\n",
    "n = 1\n",
    "\n",
    "for _, TI in enumerate(TI_list) : \n",
    "    filter_col = [col for col in df_TI if col.startswith(TI)]\n",
    "    TI_dic[TI] = (df_TI[filter_col]\n",
    "                 .set_axis(COLUMN_NAMES, axis=1)\n",
    "                 .set_index(df_TI['Dates'])\n",
    "                 .iloc[:-1]\n",
    "                 .fillna(method = 'ffill')\n",
    "                 .set_index(FR_index)\n",
    "                 .diff()\n",
    "                 .iloc[1:]\n",
    "                 )\n",
    "    # TI_dic[TI] = (TI_dic[TI] - TI_dic[TI].mean()) / TI_dic[TI].std()\n",
    "TI_dic.pop('ROC', None)\n",
    "TI_dic.pop('WLPR', None)\n",
    "TI_dic.pop('BB_PERCENT', None)\n",
    "TI_dic.pop('RSI', None)\n",
    "TI_dic.pop('HURST', None)\n",
    "TI_dic.pop('MOM', None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back_duration = 2\n",
    "look_forward = 1\n",
    "n_stocks = 8\n",
    "n_features = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zackx\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3364: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.715216372526636e-16\n",
      "9.514680406070247e-16\n",
      "4.3765328793639866e-16\n",
      "1.1821332166356727e-15\n",
      "1.4788943318270067e-15\n",
      "5.226083466544888e-16\n",
      "6.352126024143194e-16\n",
      "6.791779513951729e-16\n",
      "7.86504433126352e-16\n",
      "6.030779416076803e-16\n",
      "4.844559333858836e-16\n",
      "8.094723974451956e-16\n",
      "1.327207567085856e-15\n",
      "1.7842285702488498e-15\n",
      "5.679136249563369e-16\n",
      "6.928739070226782e-16\n",
      "4.714348462519595e-16\n",
      "1.122092324559003e-15\n",
      "1.1297994529393263e-15\n",
      "1.5058092795796271e-15\n",
      "2.6306788129175104e-15\n",
      "4.623865535409106e-16\n",
      "4.556203278501101e-16\n",
      "7.194308056294845e-16\n",
      "4.591840386007878e-16\n",
      "1.0514950513740777e-15\n",
      "1.7825632728204173e-15\n",
      "1.2227926247247051e-15\n",
      "8.116528607553997e-16\n",
      "1.3904631862370457e-15\n",
      "1.4080725960935107e-15\n",
      "1.7765832768181495e-16\n",
      "4.50340577730404e-16\n",
      "1.2161687443399763e-15\n",
      "7.822146537017316e-16\n",
      "2.856191093186113e-16\n",
      "9.519290756372705e-16\n",
      "7.161247998848347e-16\n",
      "4.4748200908758835e-16\n",
      "9.119940679617555e-16\n",
      "2.9595289406786744e-16\n",
      "7.078921987417558e-16\n",
      "3.7531773110398596e-16\n",
      "5.693173903238284e-16\n",
      "3.490874953453636e-16\n",
      "2.475587139387166e-16\n",
      "1.4404329030952597e-15\n",
      "2.0973759814108822e-16\n",
      "3.2747726127938664e-16\n",
      "9.15345230896468e-16\n",
      "3.9079662534733317e-16\n",
      "5.8064902501616135e-16\n",
      "5.572783101753951e-16\n",
      "7.827036192835198e-16\n",
      "8.725001435617168e-16\n",
      "9.58058382694286e-16\n",
      "4.0039587160120606e-16\n",
      "8.370574662805653e-16\n",
      "6.396012764470012e-16\n",
      "6.480364464722388e-16\n",
      "2.9941534878337645e-16\n",
      "4.3412202686829293e-16\n",
      "7.201727297869706e-16\n",
      "6.592558596693384e-16\n",
      "9.842233672238715e-16\n",
      "1.0519896761018977e-15\n",
      "4.943605415808689e-16\n",
      "4.2785320525878785e-16\n",
      "5.464026439137309e-16\n",
      "8.321953546938692e-16\n",
      "5.139578326428413e-16\n",
      "3.688318390707061e-16\n",
      "6.1214535230901e-16\n",
      "7.924546747507216e-16\n",
      "4.073791683610504e-16\n",
      "5.2916637197431e-16\n",
      "4.91339107445531e-16\n",
      "4.408838009039377e-16\n",
      "4.0681373524072255e-16\n",
      "3.1888513845613016e-16\n",
      "4.021672842752098e-16\n",
      "7.159268924591005e-16\n",
      "8.522557118263474e-16\n",
      "6.104668090230284e-16\n",
      "7.934641380921748e-16\n",
      "7.491141163937014e-16\n",
      "2.9726165893672137e-16\n",
      "3.910180401029449e-16\n",
      "6.157652321052505e-16\n",
      "2.6677690559407323e-16\n",
      "1.1778279030540914e-15\n",
      "2.6113860720931427e-16\n",
      "7.634328956418299e-16\n",
      "7.42664061669925e-16\n",
      "2.448880259362055e-16\n",
      "1.341705127415419e-15\n",
      "1.0966729076619855e-15\n",
      "4.610420067255613e-16\n",
      "1.0420009926679972e-15\n",
      "8.067151440416688e-16\n",
      "2.7039514966640734e-16\n",
      "3.740537276605336e-16\n",
      "1.735885692491289e-16\n",
      "9.131753212581752e-16\n",
      "5.403474754170771e-16\n",
      "7.355956637862421e-16\n",
      "8.620296230010428e-16\n",
      "4.986386573238209e-16\n",
      "3.9920024371772234e-16\n",
      "1.9455773256100633e-15\n",
      "1.1052877077322287e-15\n",
      "1.0304934251775017e-15\n",
      "8.076446436336772e-16\n",
      "3.7660264430036214e-16\n",
      "4.0283228061911906e-16\n",
      "7.980205926104266e-16\n",
      "1.2254416239367434e-15\n",
      "9.204306781353045e-16\n",
      "2.0065442272414162e-15\n",
      "5.437684067410824e-16\n",
      "3.225926653086708e-16\n",
      "1.4801786734681204e-15\n",
      "2.1919469884822527e-15\n",
      "4.732181967241365e-16\n",
      "1.1071056127723027e-15\n",
      "6.189942552955707e-16\n",
      "5.459561584161003e-16\n",
      "1.7633095466410423e-16\n",
      "4.50485163427597e-16\n",
      "6.068833811370701e-16\n",
      "4.653512392388997e-16\n",
      "4.0881972433650256e-16\n",
      "7.580597175673872e-16\n",
      "9.03165299795268e-16\n",
      "1.5688872712313556e-16\n",
      "2.7452435510546087e-16\n",
      "8.586810122886397e-16\n",
      "6.273284112805548e-16\n",
      "3.219153797461353e-16\n",
      "1.33855743367564e-15\n",
      "3.3247245494288806e-16\n",
      "5.217521587014282e-16\n",
      "4.3100127409224012e-16\n",
      "4.3094493798459164e-16\n",
      "2.281727504919283e-16\n",
      "8.407074503318137e-16\n",
      "1.1746203932731948e-15\n",
      "1.50077420261092e-15\n",
      "6.238875677340962e-16\n",
      "2.2359447065312975e-15\n",
      "7.450641155070145e-16\n",
      "6.749734113677502e-16\n",
      "1.0346684150968836e-15\n",
      "1.1006012055802221e-15\n",
      "9.355405655041503e-16\n",
      "7.744113782642681e-16\n",
      "8.097267752974397e-16\n",
      "7.488870250677682e-16\n",
      "6.835913867790936e-16\n",
      "3.967820348337078e-16\n",
      "7.824281611310641e-16\n",
      "1.2203017911410609e-15\n",
      "1.0707794101978044e-15\n",
      "5.679177913740923e-16\n",
      "7.700317847254916e-16\n",
      "1.0131384740801136e-15\n",
      "4.91405277835747e-16\n",
      "6.725052858822028e-16\n",
      "1.2748663873659183e-15\n",
      "4.905563904837265e-16\n",
      "3.6029487562804465e-16\n",
      "1.549778923398112e-15\n",
      "3.0626172613192385e-16\n",
      "6.629411258301237e-16\n",
      "7.866549545986615e-16\n",
      "1.001557336182973e-15\n",
      "8.000302961347172e-16\n",
      "5.758364536527165e-16\n",
      "9.59040927776635e-16\n",
      "3.4934172607060175e-16\n",
      "4.940605362358849e-16\n",
      "9.657848830518648e-16\n",
      "7.06241879647929e-16\n",
      "5.128992496065765e-16\n",
      "7.869520067537155e-16\n",
      "3.5943929990844994e-16\n",
      "8.811250112096176e-16\n",
      "1.3742756182316055e-15\n",
      "5.6685744216772535e-16\n",
      "4.219143978276715e-16\n",
      "7.110572912230941e-16\n",
      "9.79539970853621e-16\n",
      "5.334692696722143e-16\n",
      "4.073022901975383e-16\n",
      "1.0318946886606041e-15\n",
      "1.2506384079616148e-15\n",
      "1.2142598426125071e-15\n",
      "1.6158404722928052e-15\n",
      "5.789458818805698e-16\n",
      "1.0347315704041994e-15\n",
      "4.3292464169365744e-16\n",
      "8.009888949372849e-16\n",
      "8.365779670097771e-16\n",
      "8.756744200854693e-16\n",
      "1.3283284737634528e-15\n",
      "5.178899037003476e-16\n",
      "3.1158516741618253e-16\n",
      "1.8478854765068804e-16\n",
      "2.9881954915828923e-16\n",
      "1.0974320679176088e-15\n",
      "6.035875594505437e-16\n",
      "1.8724991719948635e-16\n",
      "7.218573499852372e-16\n",
      "1.1893160304224304e-15\n",
      "4.447421084867732e-16\n",
      "4.3427317341446e-16\n",
      "3.635234379551215e-16\n",
      "5.309507988258216e-16\n",
      "8.653822137268432e-16\n",
      "3.948709590589836e-16\n",
      "5.338501002654827e-16\n",
      "6.156921892342737e-16\n",
      "1.2404106444031031e-15\n",
      "7.213377866066191e-16\n",
      "1.6478481761224735e-16\n",
      "7.747457660195103e-16\n",
      "5.809039981318702e-16\n",
      "5.97042532669858e-16\n",
      "8.267685068649969e-16\n",
      "3.29628193747326e-16\n",
      "4.842448515703134e-16\n",
      "3.2842694545416483e-16\n",
      "3.196851629426854e-16\n",
      "1.036838931397934e-15\n",
      "9.482366597928816e-16\n",
      "2.219591084933099e-15\n",
      "8.942580429948359e-16\n",
      "5.343491850902808e-16\n",
      "6.550010848014444e-16\n",
      "2.0431028183510917e-15\n",
      "6.688230690415814e-16\n",
      "7.23360928681724e-16\n",
      "6.519299665559502e-16\n",
      "5.931340261435041e-16\n",
      "6.738333021348771e-16\n",
      "8.948229633670592e-16\n",
      "1.0787466566418095e-15\n",
      "7.738640236878555e-16\n",
      "1.453375142039495e-15\n",
      "1.891943020615872e-15\n",
      "1.1114461601752792e-15\n",
      "2.338960242696812e-15\n",
      "3.03280605780295e-16\n",
      "1.1062135090813262e-15\n",
      "1.4265600745658143e-15\n",
      "4.460716875976566e-16\n",
      "8.957061642716694e-16\n",
      "3.279207533301737e-16\n",
      "6.051835557884569e-16\n",
      "4.684632685659387e-16\n",
      "5.768304205783326e-16\n",
      "7.929527137044393e-16\n",
      "4.913372143859807e-16\n",
      "8.366946773512726e-16\n",
      "1.6654828124290344e-15\n",
      "1.0459194288047528e-15\n",
      "5.769682017468021e-16\n",
      "8.384465927531496e-16\n",
      "8.011345106969167e-16\n",
      "1.0411264093812012e-15\n",
      "1.6353115979162208e-15\n",
      "6.620135893380734e-16\n",
      "8.658672142382346e-16\n",
      "1.0065454181206947e-15\n",
      "6.168242397104943e-16\n",
      "8.345448830560187e-16\n",
      "6.156842549587867e-16\n",
      "3.975726261121176e-16\n",
      "2.926146281831752e-16\n",
      "1.0053898202188491e-15\n",
      "3.460145300088303e-16\n",
      "6.088560044658596e-16\n",
      "8.735709544390353e-16\n",
      "1.7074432197957373e-15\n",
      "4.0331221564202212e-16\n",
      "1.035287763752956e-15\n",
      "4.921946953738549e-16\n",
      "5.868026460675241e-16\n",
      "1.1246879051118685e-15\n",
      "4.273158756756447e-16\n",
      "9.442110201863844e-16\n",
      "5.740302809842641e-16\n",
      "8.116217623999744e-16\n",
      "1.603461529638069e-15\n",
      "4.929477705730399e-16\n",
      "8.292624347491457e-16\n",
      "5.160338767078296e-16\n",
      "2.78069781222779e-16\n",
      "4.854928683643936e-16\n",
      "1.8580798179754887e-15\n",
      "4.18844701398164e-16\n",
      "1.0712368158597217e-15\n",
      "3.2034752994123664e-16\n",
      "7.830140837991074e-16\n",
      "8.367884526713497e-16\n",
      "5.91416169551514e-16\n",
      "4.140588911742449e-16\n",
      "6.893733343382689e-16\n",
      "3.953498602443414e-16\n",
      "1.5030993878130697e-15\n",
      "1.135372892373638e-15\n",
      "6.18286282965514e-16\n",
      "1.089185801009838e-15\n",
      "7.419345715520348e-16\n",
      "8.317672306636359e-16\n",
      "8.31526875962141e-16\n",
      "8.86841259486209e-16\n",
      "9.746561422282935e-16\n",
      "1.0424463241128611e-15\n",
      "5.321316422064269e-16\n",
      "6.29475042739614e-16\n",
      "4.817336065557919e-16\n",
      "1.0489600349701337e-15\n",
      "1.3965296499436817e-15\n",
      "1.2906205251967028e-15\n",
      "1.389159812546387e-15\n",
      "6.023702318346207e-16\n",
      "7.771573163690597e-16\n",
      "1.0062379128479002e-15\n",
      "9.112561098994445e-16\n",
      "3.146178858965266e-16\n",
      "5.664667574621496e-16\n",
      "9.92409641720174e-16\n",
      "8.743809844899619e-16\n",
      "6.08636620890127e-16\n",
      "6.359620170611121e-16\n",
      "8.888844163240923e-16\n",
      "4.816153207005571e-16\n",
      "1.0451288870912376e-15\n",
      "1.417636646381234e-15\n",
      "1.6661419388245984e-15\n",
      "5.379667910487284e-16\n",
      "7.178437687391078e-16\n",
      "8.930198191564301e-16\n",
      "1.0422512631817e-15\n",
      "6.299567441031595e-16\n",
      "3.4889488130932613e-16\n",
      "5.477915623786686e-16\n",
      "3.2191962935840545e-16\n",
      "4.3105194939406467e-16\n",
      "1.4293776275661677e-16\n",
      "5.975294274675418e-16\n",
      "6.09501081872231e-16\n",
      "6.045802637950479e-16\n",
      "9.193864637244348e-16\n",
      "5.232659202957588e-16\n",
      "6.932593841309424e-16\n",
      "6.810999518536271e-16\n",
      "1.5049031392283945e-15\n",
      "1.9579890968152307e-16\n",
      "9.255405037208458e-16\n",
      "1.4146812342738158e-15\n",
      "4.039724071737836e-16\n",
      "4.888656767281459e-16\n",
      "4.898063396392732e-16\n",
      "9.95080847937841e-16\n",
      "4.626075647608429e-16\n",
      "2.907460655092895e-16\n",
      "7.467810806600367e-16\n",
      "1.1010499393509564e-15\n",
      "1.1010378016735335e-15\n",
      "1.6979048658282021e-15\n",
      "4.599439594799098e-16\n",
      "4.376594553755934e-16\n",
      "4.2394772161902813e-16\n",
      "2.874210121391517e-16\n",
      "1.4615261387184365e-16\n",
      "2.59314813962612e-16\n",
      "1.2041517589207434e-15\n",
      "5.01638837446552e-16\n",
      "7.601541382804438e-16\n",
      "9.193300202643987e-17\n",
      "6.320271774857942e-16\n",
      "3.674352858083268e-16\n",
      "3.4754757922492216e-16\n",
      "5.871697373366988e-16\n",
      "1.7348790312572775e-15\n",
      "2.2525017661921144e-16\n",
      "4.748619678352161e-16\n",
      "1.6529122831090725e-15\n",
      "1.7717150832580744e-15\n",
      "8.651482418816013e-16\n",
      "2.9946689024113203e-16\n",
      "1.3237955868774339e-15\n",
      "3.236566490164838e-16\n",
      "4.651607320493786e-16\n",
      "8.158892422344429e-16\n",
      "1.2814699552836463e-15\n",
      "6.2856391824933e-16\n",
      "5.968559441728689e-16\n",
      "6.435446310582107e-16\n",
      "7.352723371232678e-16\n",
      "5.771591177944163e-16\n",
      "4.686055828111215e-16\n",
      "5.666633560453409e-16\n"
     ]
    }
   ],
   "source": [
    "ft1 =  np.stack(FR_dic.values(), axis=2)\n",
    "# ft2 = np.stack(TI_dic.values(), axis=2)\n",
    "feature_tensor=ft1\n",
    "# feature_tensor = np.dstack((ft1, ft2))\n",
    "\n",
    "total_pc = np.zeros(shape=[2,n_stocks,n_features])\n",
    "list_of_features = [] #Split the features into different look_back_duration time slots. \n",
    "\n",
    "for k in range(0,len(feature_tensor),look_back_duration): \n",
    "    core, factors = tucker(feature_tensor[k:k+look_back_duration], rank= [2,n_stocks,n_features])\n",
    "    pc = tl.tenalg.mode_dot(feature_tensor[k:k+look_back_duration], factors[1].T, mode = 1)\n",
    "    total_pc = np.vstack((total_pc,pc))\n",
    "    rec = tl.tucker_to_tensor((core,factors))\n",
    "    rec_error = tl.norm(rec - feature_tensor[k:k+look_back_duration])/tl.norm(feature_tensor[k:k+look_back_duration])\n",
    "    print(rec_error)\n",
    "total_pc = total_pc[2:]\n",
    "list_of_features = [] #Split the features into different look_back_duration time slots. \n",
    "for i in range(0,len(total_pc)-look_back_duration-look_forward+1) : \n",
    "    list_of_features.append(total_pc[i:i+look_back_duration, :,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(7) : \n",
    "#     plt.figure()\n",
    "#     for n in range(n_stocks) :\n",
    "#         plt.plot(total_pc[:,n,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solo_features(solo_df, lb_duration, lf_duration) :\n",
    "    list_of_solo_f = []\n",
    "    #SOLO FEATURES\n",
    "    for i in range(0,len(solo_df)-lb_duration-lf_duration+1) : \n",
    "        list_of_solo_f.append(solo_df[i:i+lb_duration].to_numpy())\n",
    "    return list_of_solo_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET THE RETURNS OF COMPANY X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the training data of test company \n",
    "solo_df = pd.DataFrame()\n",
    "\n",
    "test_company = 'AAPL'\n",
    "k = 0\n",
    "for key,value in FR_dic.items() : \n",
    "    if k == 0 : \n",
    "        solo_df[key] = value[f'{test_company} US Equity']\n",
    "    if k > 0 : \n",
    "        solo_df[key] = value[f'{test_company} US Equity'+ f'.{k}']\n",
    "    k+=1\n",
    "\n",
    "for key,value in TI_dic.items() : \n",
    "    solo_df[key] = value[f'{test_company} US Equity']\n",
    "\n",
    "\n",
    "for col in FR_dic['PX_LAST'] : \n",
    "    if col.startswith(test_company) : \n",
    "        y_predict = FR_dic['PX_LAST'][col]\n",
    "\n",
    "test_comp_returns = pd.DataFrame(y_predict).set_axis(['Log Returns'], axis = 1)\n",
    "test_comp_returns['Cumulative Log Returns'] = test_comp_returns['Log Returns'].cumsum()\n",
    "def get_n_week_retuns(log_returns, look_back_duration, lookforward) : \n",
    "    sdate = test_comp_returns.index.values[0]\n",
    "    edate = test_comp_returns.index.values[-1]\n",
    "    s = (log_returns\n",
    "     .reset_index()\n",
    "     .iloc[look_back_duration:]\n",
    "    )\n",
    "    n_week_retuns = s.rolling(lookforward).sum()\n",
    "    n_week_retuns.index = list(pd.date_range(sdate ,edate + pd.to_timedelta(2, unit='D') ,freq='w') - pd.to_timedelta(2, unit='D'))[:-look_back_duration]\n",
    "    return n_week_retuns.dropna() \n",
    "\n",
    "\n",
    "def to_simple_return(cum_log_ret) : \n",
    "    return np.exp(cum_log_ret) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import newaxis\n",
    "# x_ret_list contains the \n",
    "y_arr =  get_n_week_retuns(test_comp_returns['Log Returns'], look_back_duration = look_back_duration, lookforward = look_forward)\n",
    "\n",
    "y_arr[(y_arr > 0)] = 1\n",
    "\n",
    "y_arr[y_arr <= 0] = -1\n",
    "\n",
    "y_ret = y_arr.to_numpy().flatten()\n",
    "\n",
    "\n",
    "list_of_solo_f = get_solo_features(solo_df, look_back_duration, look_forward)\n",
    "list_of_combined_features = []\n",
    "\n",
    "\n",
    "for feature_t,solo_feature in zip(list_of_features,list_of_solo_f)  :\n",
    "    solo_feature = solo_feature[:,newaxis,:]\n",
    "    tmp = np.concatenate((feature_t,solo_feature), axis=1)\n",
    "    list_of_combined_features.append(tmp)\n",
    "\n",
    "\n",
    "X_Cols = [tensor.flatten() for tensor in list_of_features ]\n",
    "\n",
    "S_Cols = [mat.flatten() for mat in list_of_solo_f]\n",
    "\n",
    "C_Cols = [tensor.flatten() for tensor in list_of_combined_features]\n",
    "\n",
    "C_val, Y_val = C_Cols[-120:], y_ret[-120:]\n",
    "\n",
    "X_Cols, S_Cols, C_Cols = X_Cols[:-120], S_Cols[:-120] , C_Cols[:-120]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from numpy import newaxis\n",
    " \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_Cols, y_ret[:-120], random_state = 42, shuffle=False)\n",
    "\n",
    "S_train, S_test, Y_train, Y_test = train_test_split(S_Cols, y_ret[:-120], random_state = 42, shuffle=False)\n",
    "\n",
    "C_train, C_test, Y_train, Y_test = train_test_split(C_Cols, y_ret[:-120], random_state = 42, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, mean_squared_error, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zackx\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [False, True],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [2, 3],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 3],\n",
       "                                        &#x27;n_estimators&#x27;: [1000, 1500, 2000,\n",
       "                                                         2500]},\n",
       "                   random_state=144, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [False, True],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [2, 3],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 3],\n",
       "                                        &#x27;n_estimators&#x27;: [1000, 1500, 2000,\n",
       "                                                         2500]},\n",
       "                   random_state=144, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [False, True],\n",
       "                                        'max_depth': [3, 5, 7],\n",
       "                                        'max_features': ['auto'],\n",
       "                                        'min_samples_leaf': [2, 3],\n",
       "                                        'min_samples_split': [2, 3],\n",
       "                                        'n_estimators': [1000, 1500, 2000,\n",
       "                                                         2500]},\n",
       "                   random_state=144, verbose=4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HYPERPARAMETER TUNING\n",
    "n_estimators = [1000, 1500, 2000, 2500]\n",
    "\n",
    "max_features = ['auto']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(3,7,3)]\n",
    "\n",
    "min_samples_split = [2,3]\n",
    "\n",
    "min_samples_leaf = [2,3]\n",
    "\n",
    "bootstrap = [False, True]\n",
    "\n",
    "\n",
    "random_grid = {\n",
    "    \n",
    "'n_estimators': n_estimators,\n",
    "\n",
    "'max_features': max_features,\n",
    "\n",
    "'max_depth': max_depth,\n",
    "\n",
    "'min_samples_split': min_samples_split,\n",
    "\n",
    "'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "'bootstrap' : bootstrap\n",
    "\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Using 5 fold cross validation\n",
    "# Search across 100 different combintations and use all available cores\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf,\n",
    "    param_distributions= random_grid,\n",
    "    n_iter = 50, \n",
    "    verbose= 4,\n",
    "    cv = 4,\n",
    "    random_state= 144,\n",
    "    n_jobs = - 1,\n",
    "    \n",
    ")\n",
    "\n",
    "rf_random.fit(C_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.81141329, 4.53853073]),\n",
       " 'std_fit_time': array([0.08543424, 0.03435056]),\n",
       " 'mean_score_time': array([0.10867648, 0.15983615]),\n",
       " 'std_score_time': array([0.01287727, 0.00397086]),\n",
       " 'param_n_estimators': masked_array(data=[1500, 2500],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[2, 3],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[3, 3],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['auto', 'auto'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[4, 7],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_bootstrap': masked_array(data=[False, True],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 1500,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 3,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 4,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 2500,\n",
       "   'min_samples_split': 3,\n",
       "   'min_samples_leaf': 3,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 7,\n",
       "   'bootstrap': True}],\n",
       " 'split0_test_score': array([0.49038462, 0.42307692]),\n",
       " 'split1_test_score': array([0.53398058, 0.5631068 ]),\n",
       " 'split2_test_score': array([0.53398058, 0.48543689]),\n",
       " 'split3_test_score': array([0.51456311, 0.52427184]),\n",
       " 'split4_test_score': array([0.5631068 , 0.53398058]),\n",
       " 'mean_test_score': array([0.52720314, 0.50597461]),\n",
       " 'std_test_score': array([0.02406788, 0.04832605]),\n",
       " 'rank_test_score': array([1, 2])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random grid:  {'n_estimators': [1000, 1500, 2000, 2500], 'max_features': ['auto'], 'max_depth': [3, 5, 7], 'min_samples_split': [2, 3], 'min_samples_leaf': [2, 3], 'bootstrap': [False, True]} \n",
      "\n",
      "Best Parameters:  {'n_estimators': 1500, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 3, 'bootstrap': False}  \n",
      "\n",
      "{'n_estimators': 1500, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 3, 'bootstrap': False}\n",
      "0.7209302325581395\n",
      "[[ 77 144]\n",
      " [  0 295]]\n",
      "0.6511627906976745\n",
      "[[16 47]\n",
      " [13 96]]\n",
      "0.55\n",
      "[[11 40]\n",
      " [14 55]]\n"
     ]
    }
   ],
   "source": [
    "print('Random grid: ', random_grid, '\\n')\n",
    "\n",
    "print('Best Parameters: ', rf_random.best_params_, ' \\n')\n",
    "\n",
    "# print(rf_random.best_score_)\n",
    "print(rf_random.best_params_)\n",
    "target_names = ['Red', 'Green']\n",
    "prediction = rf_random.predict(C_train)\n",
    "print(accuracy_score(Y_train, prediction))\n",
    "# print(classification_report(Y_train, prediction, target_names=target_names))\n",
    "print(confusion_matrix(Y_train, prediction))\n",
    "\n",
    "\n",
    "prediction = rf_random.predict(C_test)\n",
    "print(accuracy_score(Y_test, prediction))\n",
    "# print(classification_report(Y_test, prediction, target_names=target_names))\n",
    "print(confusion_matrix(Y_test, prediction))\n",
    "\n",
    "\n",
    "prediction_val1= rf_random.predict(C_val)\n",
    "print(accuracy_score(Y_val, prediction_val1))\n",
    "print(confusion_matrix(Y_val, prediction_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.PX_LAST.3    0.000485\n",
      "2.PB.5         0.001306\n",
      "2.PS.6         0.001330\n",
      "1.PS.2         0.001408\n",
      "1.PE.3         0.001493\n",
      "1.PS.6         0.001661\n",
      "1.PE.5         0.001691\n",
      "2.PX_LAST.6    0.001805\n",
      "1.PX_LAST.7    0.002119\n",
      "1.PS.5         0.002452\n",
      "2.PX_LAST.8    0.002538\n",
      "2.PB.0         0.002609\n",
      "2.PB.4         0.002771\n",
      "2.PX_LAST.7    0.002881\n",
      "2.PS.8         0.003003\n",
      "2.PX_LAST.5    0.003034\n",
      "1.PS.3         0.003167\n",
      "1.PE.6         0.003414\n",
      "2.PS.7         0.003433\n",
      "1.PE.7         0.003579\n",
      "2.PB.1         0.003984\n",
      "1.PS.7         0.004013\n",
      "1.PB.7         0.004083\n",
      "1.PB.2         0.004100\n",
      "2.PE.6         0.004156\n",
      "2.PS.3         0.004173\n",
      "1.PE.8         0.004240\n",
      "2.PB.8         0.004256\n",
      "1.PX_LAST.2    0.004371\n",
      "1.PE.4         0.004471\n",
      "1.PB.1         0.004516\n",
      "1.PE.1         0.004675\n",
      "2.PS.2         0.004952\n",
      "1.PB.6         0.005256\n",
      "2.PS.4         0.005287\n",
      "1.PX_LAST.8    0.005402\n",
      "1.PS.4         0.005529\n",
      "2.PS.0         0.005800\n",
      "2.PE.4         0.006279\n",
      "1.PB.4         0.006383\n",
      "1.PX_LAST.4    0.006644\n",
      "2.PE.7         0.006996\n",
      "2.PE.5         0.007048\n",
      "2.PE.2         0.007319\n",
      "2.PX_LAST.2    0.007691\n",
      "1.PB.5         0.007782\n",
      "2.PX_LAST.3    0.008154\n",
      "1.PX_LAST.6    0.008577\n",
      "2.PX_LAST.4    0.009823\n",
      "2.PX_LAST.0    0.009899\n",
      "1.PB.0         0.010003\n",
      "1.PE.2         0.010684\n",
      "1.PB.3         0.010750\n",
      "2.PE.8         0.011568\n",
      "1.PX_LAST.0    0.014092\n",
      "2.PB.6         0.016429\n",
      "1.PX_LAST.5    0.017643\n",
      "1.PB.8         0.019784\n",
      "2.PE.0         0.021209\n",
      "2.PS.5         0.021269\n",
      "2.PB.7         0.025500\n",
      "2.PB.2         0.028346\n",
      "1.PS.0         0.029196\n",
      "1.PS.1         0.030390\n",
      "2.PX_LAST.1    0.032492\n",
      "1.PS.8         0.040622\n",
      "2.PE.3         0.049091\n",
      "2.PB.3         0.055372\n",
      "2.PS.1         0.058436\n",
      "1.PX_LAST.1    0.074719\n",
      "1.PE.0         0.099390\n",
      "2.PE.1         0.100977\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_imp = pd.Series(rf_random.best_estimator_.feature_importances_)\n",
    "feature_imp = feature_imp.sort_values(ascending=True)\n",
    "srted_indx_list = feature_imp.index.to_list()\n",
    "\n",
    "col_names= ['PE', 'PX_LAST', 'PS', 'PB', 'HURST', 'MOM', 'RSI']\n",
    "\n",
    "\n",
    "def get_feature_name (srted_indx_list) :\n",
    "    name_list = []\n",
    "    for num in srted_indx_list : \n",
    "        half = len(srted_indx_list)/2 \n",
    "        if num < half : \n",
    "            rows = num // n_features\n",
    "            cols = num % n_features\n",
    "\n",
    "            name = f'1.{col_names[cols]}.{rows}' \n",
    "            \n",
    "        else : \n",
    "            rows = num // n_features - n_stocks - 1\n",
    "            cols = num % n_features\n",
    "            name = f'2.{col_names[cols]}.{rows}' \n",
    "    \n",
    "        name_list.append(name)\n",
    "    \n",
    "    return name_list\n",
    "\n",
    "\n",
    "feature_imp.index = (get_feature_name(srted_indx_list) )\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(feature_imp)\n",
    "\n",
    "\n",
    "        #second time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.PE.1         0.100977\n",
      "1.PE.0         0.099390\n",
      "1.PX_LAST.1    0.074719\n",
      "2.PS.1         0.058436\n",
      "2.PB.3         0.055372\n",
      "2.PE.3         0.049091\n",
      "1.PS.8         0.040622\n",
      "2.PX_LAST.1    0.032492\n",
      "1.PS.1         0.030390\n",
      "1.PS.0         0.029196\n",
      "2.PB.2         0.028346\n",
      "2.PB.7         0.025500\n",
      "2.PS.5         0.021269\n",
      "2.PE.0         0.021209\n",
      "1.PB.8         0.019784\n",
      "1.PX_LAST.5    0.017643\n",
      "2.PB.6         0.016429\n",
      "1.PX_LAST.0    0.014092\n",
      "2.PE.8         0.011568\n",
      "1.PB.3         0.010750\n",
      "1.PE.2         0.010684\n",
      "1.PB.0         0.010003\n",
      "2.PX_LAST.0    0.009899\n",
      "2.PX_LAST.4    0.009823\n",
      "1.PX_LAST.6    0.008577\n",
      "2.PX_LAST.3    0.008154\n",
      "1.PB.5         0.007782\n",
      "2.PX_LAST.2    0.007691\n",
      "2.PE.2         0.007319\n",
      "2.PE.5         0.007048\n",
      "2.PE.7         0.006996\n",
      "1.PX_LAST.4    0.006644\n",
      "1.PB.4         0.006383\n",
      "2.PE.4         0.006279\n",
      "2.PS.0         0.005800\n",
      "1.PS.4         0.005529\n",
      "1.PX_LAST.8    0.005402\n",
      "2.PS.4         0.005287\n",
      "1.PB.6         0.005256\n",
      "2.PS.2         0.004952\n",
      "1.PE.1         0.004675\n",
      "1.PB.1         0.004516\n",
      "1.PE.4         0.004471\n",
      "1.PX_LAST.2    0.004371\n",
      "2.PB.8         0.004256\n",
      "1.PE.8         0.004240\n",
      "2.PS.3         0.004173\n",
      "2.PE.6         0.004156\n",
      "1.PB.2         0.004100\n",
      "1.PB.7         0.004083\n",
      "1.PS.7         0.004013\n",
      "2.PB.1         0.003984\n",
      "1.PE.7         0.003579\n",
      "2.PS.7         0.003433\n",
      "1.PE.6         0.003414\n",
      "1.PS.3         0.003167\n",
      "2.PX_LAST.5    0.003034\n",
      "2.PS.8         0.003003\n",
      "2.PX_LAST.7    0.002881\n",
      "2.PB.4         0.002771\n",
      "2.PB.0         0.002609\n",
      "2.PX_LAST.8    0.002538\n",
      "1.PS.5         0.002452\n",
      "1.PX_LAST.7    0.002119\n",
      "2.PX_LAST.6    0.001805\n",
      "1.PE.5         0.001691\n",
      "1.PS.6         0.001661\n",
      "1.PE.3         0.001493\n",
      "1.PS.2         0.001408\n",
      "2.PS.6         0.001330\n",
      "2.PB.5         0.001306\n",
      "1.PX_LAST.3    0.000485\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(feature_imp.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_137044/777554630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Baseline if all were -1 : {np.count_nonzero((all_neg_ones==Y_test)) / len(Y_test)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mbaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# baseline(Y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_137044/777554630.py\u001b[0m in \u001b[0;36mbaseline\u001b[1;34m(Y_test, prediction)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mall_ones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "\n",
    "def baseline(Y_test, prediction) : \n",
    "    Y_test = np.array(Y_test.values)\n",
    "    Y_test = Y_test.flatten()\n",
    "\n",
    "    all_ones = np.ones(len(Y_test))\n",
    "    all_neg_ones = np.ones(len(Y_test))* -1\n",
    "    print(f\"Actual number of ones : {np.count_nonzero(Y_test == 1)}\")\n",
    "    print(f\"Predicted number of ones : {np.count_nonzero(prediction == 1)}\")\n",
    "    print(f\"Baseline if all were ones : {np.count_nonzero((all_ones==Y_test)) / len(Y_test)}\")\n",
    "\n",
    "    print(f\"Actual number of -1 : {np.count_nonzero(Y_test == -1)}\")\n",
    "    print(f\"Predicted number of -1 : {np.count_nonzero(prediction == -1)}\")\n",
    "    print(f\"Baseline if all were -1 : {np.count_nonzero((all_neg_ones==Y_test)) / len(Y_test)}\")\n",
    "\n",
    "baseline(Y_test,prediction)\n",
    "\n",
    "# baseline(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FR_dic['PE']['AAPL US Equity'].rolling(50).mean().plot()\n",
    "# # FR_dic['PE']['SLB US Equity'].rolling(50).mean().plot()\n",
    "# aapl_acf = acf(FR_dic['PE']['AAPL US Equity'])\n",
    "# aapl_acf_2 = acf(FR_dic['PX_LAST']['AAPL US Equity.1'])\n",
    "# aapl_acf_3 = acf(FR_dic['PS']['AAPL US Equity.2'])\n",
    "# aapl_acf_4 = acf(FR_dic['PB']['AAPL US Equity.3'])\n",
    "# aapl_acf_5 = acf(TI_dic['HURST']['AAPL US Equity'])\n",
    "# aapl_acf_6 = acf(TI_dic['HURST']['AAPL US Equity'])\n",
    "\n",
    "# # aapl_acf_6 = acf(FR_dic['PB']['AAPL US Equity.3'])\n",
    "\n",
    "\n",
    "# test_df = pd.DataFrame([aapl_acf, aapl_acf_2, aapl_acf_3, aapl_acf_4, aapl_acf_5, aapl_acf_6]).T\n",
    "# test_df.columns = ['ACF of first-order difference of PE of AAPL', 'ACF of first-order difference of PX_LAST of AAPL', \n",
    "#                    'ACF of first-order difference of PS of AAPL', 'ACF of first-order difference of PB of AAPL', 'ACF of first-order difference of HURST of AAPL',\n",
    "#                    'ACF of first-order difference of Momentum of AAPL'\n",
    "#                   ] \n",
    "# test_df.index += 1\n",
    "# test_df.plot(kind='bar', figsize=(10,7))\n",
    "\n",
    "# plt.savefig('ACF.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e2c2ee5cf96267dcc7744f326d3a9b930733ff27582ed4f98f71c09c9039794"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
