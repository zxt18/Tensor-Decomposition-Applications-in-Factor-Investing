{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorly.decomposition import CP,tucker, parafac, non_negative_tucker\n",
    "from datetime import timedelta, date\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR = pd.read_excel('FINANCIAL_RATIOS_NEW.xlsx', index_col =[0], skiprows=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = None\n",
    "FR_dic = {}\n",
    "FR_list = ['PE', 'PX_LAST', 'PS','PB']\n",
    "n = 0 \n",
    "NUMBER_OF_STOCKS = 220\n",
    "for i,name in enumerate(FR_list) : \n",
    "    FR_dic[name] = (df_FR.iloc[:,n:n+NUMBER_OF_STOCKS]\n",
    "                    .set_index(df_FR.index)\n",
    "                    )\n",
    "\n",
    "    if name not in ['PX_LAST', 'MCAP'] : \n",
    "        # FR_dic[name] = (FR_dic[name] - FR_dic[name].mean()) / FR_dic[name].std()\n",
    "        FR_dic[name] = FR_dic[name].diff()\n",
    "    if name in ['PX_LAST'] :\n",
    "\n",
    "        price = FR_dic['PX_LAST']\n",
    "        FR_dic[name] = np.log(FR_dic[name])\n",
    "        FR_dic[name] = FR_dic[name].diff()\n",
    "    \n",
    "\n",
    "    \n",
    "    FR_dic[name] = FR_dic[name][1:]\n",
    "    n += NUMBER_OF_STOCKS + 2\n",
    "\n",
    "COLUMN_NAMES = FR_dic['PE'].columns\n",
    "FR_index = FR_dic['PE'].index\n",
    "\n",
    "for key, val in FR_dic.items() : \n",
    "    FR_dic[key] = FR_dic[key][1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMAT US Equity</th>\n",
       "      <th>COKE US Equity</th>\n",
       "      <th>WDFC US Equity</th>\n",
       "      <th>AAPL US Equity</th>\n",
       "      <th>KLAC US Equity</th>\n",
       "      <th>SEIC US Equity</th>\n",
       "      <th>CSPI US Equity</th>\n",
       "      <th>ALOT US Equity</th>\n",
       "      <th>AMGN US Equity</th>\n",
       "      <th>CAMP US Equity</th>\n",
       "      <th>...</th>\n",
       "      <th>DVN US Equity</th>\n",
       "      <th>ORCL US Equity</th>\n",
       "      <th>PG US Equity</th>\n",
       "      <th>INTC US Equity</th>\n",
       "      <th>TTMI US Equity</th>\n",
       "      <th>TTEK US Equity</th>\n",
       "      <th>WFC US Equity</th>\n",
       "      <th>WMT US Equity</th>\n",
       "      <th>C US Equity</th>\n",
       "      <th>BA US Equity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-11-24</th>\n",
       "      <td>-2.224233</td>\n",
       "      <td>-0.453817</td>\n",
       "      <td>-1.517323</td>\n",
       "      <td>4.841130</td>\n",
       "      <td>3.387437</td>\n",
       "      <td>1.096661</td>\n",
       "      <td>-1.312913</td>\n",
       "      <td>2.897439</td>\n",
       "      <td>-3.120883</td>\n",
       "      <td>-1.105011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146298</td>\n",
       "      <td>2.252991</td>\n",
       "      <td>1.893400</td>\n",
       "      <td>-4.501691</td>\n",
       "      <td>1.649787</td>\n",
       "      <td>-4.532297</td>\n",
       "      <td>-5.840585</td>\n",
       "      <td>1.202504</td>\n",
       "      <td>1.595611</td>\n",
       "      <td>3.793508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-01</th>\n",
       "      <td>-2.053757</td>\n",
       "      <td>-1.161186</td>\n",
       "      <td>2.554492</td>\n",
       "      <td>1.029270</td>\n",
       "      <td>-3.785001</td>\n",
       "      <td>-3.176736</td>\n",
       "      <td>0.854699</td>\n",
       "      <td>0.794800</td>\n",
       "      <td>-0.874210</td>\n",
       "      <td>0.311577</td>\n",
       "      <td>...</td>\n",
       "      <td>2.428829</td>\n",
       "      <td>-7.622190</td>\n",
       "      <td>-2.923864</td>\n",
       "      <td>-3.503785</td>\n",
       "      <td>-2.067527</td>\n",
       "      <td>-4.515547</td>\n",
       "      <td>-0.390235</td>\n",
       "      <td>-4.191311</td>\n",
       "      <td>-6.367649</td>\n",
       "      <td>-2.825137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-08</th>\n",
       "      <td>2.827996</td>\n",
       "      <td>3.392074</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>-3.802464</td>\n",
       "      <td>1.560281</td>\n",
       "      <td>1.693252</td>\n",
       "      <td>3.191663</td>\n",
       "      <td>4.231011</td>\n",
       "      <td>-7.711691</td>\n",
       "      <td>2.237694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621117</td>\n",
       "      <td>-14.751177</td>\n",
       "      <td>2.297295</td>\n",
       "      <td>-2.359614</td>\n",
       "      <td>1.087782</td>\n",
       "      <td>1.348066</td>\n",
       "      <td>0.453615</td>\n",
       "      <td>-1.242478</td>\n",
       "      <td>5.029320</td>\n",
       "      <td>2.456348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-15</th>\n",
       "      <td>-5.463387</td>\n",
       "      <td>2.190771</td>\n",
       "      <td>0.619418</td>\n",
       "      <td>-1.499352</td>\n",
       "      <td>-5.481471</td>\n",
       "      <td>-1.666538</td>\n",
       "      <td>-7.922685</td>\n",
       "      <td>-2.253118</td>\n",
       "      <td>-0.139993</td>\n",
       "      <td>-0.104012</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.614273</td>\n",
       "      <td>0.986996</td>\n",
       "      <td>-2.805150</td>\n",
       "      <td>-1.394327</td>\n",
       "      <td>-6.813869</td>\n",
       "      <td>1.650935</td>\n",
       "      <td>-1.483038</td>\n",
       "      <td>-2.269033</td>\n",
       "      <td>6.812452</td>\n",
       "      <td>-2.949547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-22</th>\n",
       "      <td>2.563554</td>\n",
       "      <td>-1.523547</td>\n",
       "      <td>5.373134</td>\n",
       "      <td>-7.671856</td>\n",
       "      <td>1.015592</td>\n",
       "      <td>1.766941</td>\n",
       "      <td>-2.424392</td>\n",
       "      <td>4.289810</td>\n",
       "      <td>-1.142916</td>\n",
       "      <td>12.671741</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.660249</td>\n",
       "      <td>-8.413056</td>\n",
       "      <td>4.168915</td>\n",
       "      <td>-0.721105</td>\n",
       "      <td>-0.109521</td>\n",
       "      <td>-2.300894</td>\n",
       "      <td>4.215124</td>\n",
       "      <td>-0.108761</td>\n",
       "      <td>8.626871</td>\n",
       "      <td>0.697624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-29</th>\n",
       "      <td>-4.319681</td>\n",
       "      <td>-5.418509</td>\n",
       "      <td>-1.822813</td>\n",
       "      <td>-7.034069</td>\n",
       "      <td>-6.333237</td>\n",
       "      <td>-5.052269</td>\n",
       "      <td>-6.201765</td>\n",
       "      <td>1.361242</td>\n",
       "      <td>-6.199239</td>\n",
       "      <td>-3.127722</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.947924</td>\n",
       "      <td>-5.867009</td>\n",
       "      <td>-7.145496</td>\n",
       "      <td>-4.898988</td>\n",
       "      <td>-5.522428</td>\n",
       "      <td>-6.414245</td>\n",
       "      <td>-7.175406</td>\n",
       "      <td>-6.748935</td>\n",
       "      <td>-4.132088</td>\n",
       "      <td>-8.527885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-06</th>\n",
       "      <td>7.752239</td>\n",
       "      <td>6.566924</td>\n",
       "      <td>-1.650809</td>\n",
       "      <td>6.288856</td>\n",
       "      <td>8.973904</td>\n",
       "      <td>2.436015</td>\n",
       "      <td>6.915645</td>\n",
       "      <td>2.174468</td>\n",
       "      <td>-9.821068</td>\n",
       "      <td>6.042185</td>\n",
       "      <td>...</td>\n",
       "      <td>8.255261</td>\n",
       "      <td>0.659840</td>\n",
       "      <td>-3.021952</td>\n",
       "      <td>2.938458</td>\n",
       "      <td>6.078284</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>3.360934</td>\n",
       "      <td>0.316319</td>\n",
       "      <td>5.990293</td>\n",
       "      <td>1.203183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-13</th>\n",
       "      <td>-6.643218</td>\n",
       "      <td>0.363220</td>\n",
       "      <td>-2.254292</td>\n",
       "      <td>-10.396870</td>\n",
       "      <td>-6.947908</td>\n",
       "      <td>-8.384277</td>\n",
       "      <td>-4.233947</td>\n",
       "      <td>-2.582282</td>\n",
       "      <td>2.783983</td>\n",
       "      <td>-4.938289</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.276851</td>\n",
       "      <td>-4.063099</td>\n",
       "      <td>-6.452394</td>\n",
       "      <td>-5.764848</td>\n",
       "      <td>-3.898360</td>\n",
       "      <td>-8.218085</td>\n",
       "      <td>-5.282535</td>\n",
       "      <td>-7.891160</td>\n",
       "      <td>-5.174845</td>\n",
       "      <td>-6.269296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>5.084828</td>\n",
       "      <td>-1.738245</td>\n",
       "      <td>-0.181066</td>\n",
       "      <td>-2.442297</td>\n",
       "      <td>5.049160</td>\n",
       "      <td>4.358786</td>\n",
       "      <td>7.519199</td>\n",
       "      <td>-0.412611</td>\n",
       "      <td>1.975760</td>\n",
       "      <td>9.140310</td>\n",
       "      <td>...</td>\n",
       "      <td>3.926319</td>\n",
       "      <td>-2.405455</td>\n",
       "      <td>-8.106560</td>\n",
       "      <td>-0.685424</td>\n",
       "      <td>3.508007</td>\n",
       "      <td>0.625243</td>\n",
       "      <td>-0.658935</td>\n",
       "      <td>-17.502356</td>\n",
       "      <td>3.534116</td>\n",
       "      <td>-0.758714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-27</th>\n",
       "      <td>-1.249969</td>\n",
       "      <td>14.230107</td>\n",
       "      <td>4.290616</td>\n",
       "      <td>-0.130176</td>\n",
       "      <td>0.909415</td>\n",
       "      <td>2.092628</td>\n",
       "      <td>3.644795</td>\n",
       "      <td>-0.859436</td>\n",
       "      <td>4.918664</td>\n",
       "      <td>6.779078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.397239</td>\n",
       "      <td>2.489456</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>-0.225175</td>\n",
       "      <td>-4.222273</td>\n",
       "      <td>1.629949</td>\n",
       "      <td>4.972942</td>\n",
       "      <td>0.889768</td>\n",
       "      <td>6.597556</td>\n",
       "      <td>-0.693949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AMAT US Equity  COKE US Equity  WDFC US Equity  AAPL US Equity  \\\n",
       "2006-11-24       -2.224233       -0.453817       -1.517323        4.841130   \n",
       "2006-12-01       -2.053757       -1.161186        2.554492        1.029270   \n",
       "2006-12-08        2.827996        3.392074        0.319400       -3.802464   \n",
       "2006-12-15       -5.463387        2.190771        0.619418       -1.499352   \n",
       "2006-12-22        2.563554       -1.523547        5.373134       -7.671856   \n",
       "...                    ...             ...             ...             ...   \n",
       "2022-04-29       -4.319681       -5.418509       -1.822813       -7.034069   \n",
       "2022-05-06        7.752239        6.566924       -1.650809        6.288856   \n",
       "2022-05-13       -6.643218        0.363220       -2.254292      -10.396870   \n",
       "2022-05-20        5.084828       -1.738245       -0.181066       -2.442297   \n",
       "2022-05-27       -1.249969       14.230107        4.290616       -0.130176   \n",
       "\n",
       "            KLAC US Equity  SEIC US Equity  CSPI US Equity  ALOT US Equity  \\\n",
       "2006-11-24        3.387437        1.096661       -1.312913        2.897439   \n",
       "2006-12-01       -3.785001       -3.176736        0.854699        0.794800   \n",
       "2006-12-08        1.560281        1.693252        3.191663        4.231011   \n",
       "2006-12-15       -5.481471       -1.666538       -7.922685       -2.253118   \n",
       "2006-12-22        1.015592        1.766941       -2.424392        4.289810   \n",
       "...                    ...             ...             ...             ...   \n",
       "2022-04-29       -6.333237       -5.052269       -6.201765        1.361242   \n",
       "2022-05-06        8.973904        2.436015        6.915645        2.174468   \n",
       "2022-05-13       -6.947908       -8.384277       -4.233947       -2.582282   \n",
       "2022-05-20        5.049160        4.358786        7.519199       -0.412611   \n",
       "2022-05-27        0.909415        2.092628        3.644795       -0.859436   \n",
       "\n",
       "            AMGN US Equity  CAMP US Equity  ...  DVN US Equity  \\\n",
       "2006-11-24       -3.120883       -1.105011  ...       0.146298   \n",
       "2006-12-01       -0.874210        0.311577  ...       2.428829   \n",
       "2006-12-08       -7.711691        2.237694  ...       0.621117   \n",
       "2006-12-15       -0.139993       -0.104012  ...      -4.614273   \n",
       "2006-12-22       -1.142916       12.671741  ...      -4.660249   \n",
       "...                    ...             ...  ...            ...   \n",
       "2022-04-29       -6.199239       -3.127722  ...     -10.947924   \n",
       "2022-05-06       -9.821068        6.042185  ...       8.255261   \n",
       "2022-05-13        2.783983       -4.938289  ...      -4.276851   \n",
       "2022-05-20        1.975760        9.140310  ...       3.926319   \n",
       "2022-05-27        4.918664        6.779078  ...       1.397239   \n",
       "\n",
       "            ORCL US Equity  PG US Equity  INTC US Equity  TTMI US Equity  \\\n",
       "2006-11-24        2.252991      1.893400       -4.501691        1.649787   \n",
       "2006-12-01       -7.622190     -2.923864       -3.503785       -2.067527   \n",
       "2006-12-08      -14.751177      2.297295       -2.359614        1.087782   \n",
       "2006-12-15        0.986996     -2.805150       -1.394327       -6.813869   \n",
       "2006-12-22       -8.413056      4.168915       -0.721105       -0.109521   \n",
       "...                    ...           ...             ...             ...   \n",
       "2022-04-29       -5.867009     -7.145496       -4.898988       -5.522428   \n",
       "2022-05-06        0.659840     -3.021952        2.938458        6.078284   \n",
       "2022-05-13       -4.063099     -6.452394       -5.764848       -3.898360   \n",
       "2022-05-20       -2.405455     -8.106560       -0.685424        3.508007   \n",
       "2022-05-27        2.489456      0.245620       -0.225175       -4.222273   \n",
       "\n",
       "            TTEK US Equity  WFC US Equity  WMT US Equity  C US Equity  \\\n",
       "2006-11-24       -4.532297      -5.840585       1.202504     1.595611   \n",
       "2006-12-01       -4.515547      -0.390235      -4.191311    -6.367649   \n",
       "2006-12-08        1.348066       0.453615      -1.242478     5.029320   \n",
       "2006-12-15        1.650935      -1.483038      -2.269033     6.812452   \n",
       "2006-12-22       -2.300894       4.215124      -0.108761     8.626871   \n",
       "...                    ...            ...            ...          ...   \n",
       "2022-04-29       -6.414245      -7.175406      -6.748935    -4.132088   \n",
       "2022-05-06        0.128834       3.360934       0.316319     5.990293   \n",
       "2022-05-13       -8.218085      -5.282535      -7.891160    -5.174845   \n",
       "2022-05-20        0.625243      -0.658935     -17.502356     3.534116   \n",
       "2022-05-27        1.629949       4.972942       0.889768     6.597556   \n",
       "\n",
       "            BA US Equity  \n",
       "2006-11-24      3.793508  \n",
       "2006-12-01     -2.825137  \n",
       "2006-12-08      2.456348  \n",
       "2006-12-15     -2.949547  \n",
       "2006-12-22      0.697624  \n",
       "...                  ...  \n",
       "2022-04-29     -8.527885  \n",
       "2022-05-06      1.203183  \n",
       "2022-05-13     -6.269296  \n",
       "2022-05-20     -0.758714  \n",
       "2022-05-27     -0.693949  \n",
       "\n",
       "[810 rows x 220 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TI = pd.read_excel('TECH_NEW.xlsx', skiprows=[0])\n",
    "\n",
    "TI_dic = {}\n",
    "TI_list = ['BB_PERCENT', 'HURST', 'MOM', 'ROC','RSI','WLPR']\n",
    "n = 1\n",
    "\n",
    "for _, TI in enumerate(TI_list) : \n",
    "    filter_col = [col for col in df_TI if col.startswith(TI)]\n",
    "    TI_dic[TI] = (df_TI[filter_col]\n",
    "                 .set_axis(COLUMN_NAMES, axis=1)\n",
    "                 .set_index(df_TI['Dates'])\n",
    "                 .iloc[:-1]\n",
    "                 .fillna(method = 'ffill')\n",
    "                 .set_index(FR_index)\n",
    "                 .diff()\n",
    "                 .iloc[1:]\n",
    "                 )\n",
    "    # TI_dic[TI] = (TI_dic[TI] - TI_dic[TI].mean()) / TI_dic[TI].std()\n",
    "TI_dic.pop('ROC', None)\n",
    "TI_dic.pop('WLPR', None)\n",
    "TI_dic.pop('BB_PERCENT', None)\n",
    "TI_dic.pop('RSI', None)\n",
    "# TI_dic.pop('HURST', None)\n",
    "# TI_dic.pop('MOM', None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back_duration = 2\n",
    "look_forward = 1\n",
    "n_stocks = 12\n",
    "n_features = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zackx\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3364: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3189271372945887e-15\n",
      "7.648126856106465e-16\n",
      "5.857458078350206e-16\n",
      "1.6928216651164426e-15\n",
      "3.9701249856705404e-16\n",
      "1.223581860763359e-15\n",
      "1.0420200992642226e-15\n",
      "1.7477124423734366e-15\n",
      "1.0016326569229115e-15\n",
      "4.154641467524285e-16\n",
      "8.963953887680022e-16\n",
      "1.7752101749319097e-15\n",
      "1.0747992758363487e-15\n",
      "1.4601792930706108e-15\n",
      "2.2887259500419654e-15\n",
      "1.0751775404845096e-15\n",
      "1.2343153703020915e-15\n",
      "8.323645289694882e-16\n",
      "9.40342502854075e-16\n",
      "1.296311231305445e-15\n",
      "1.1764840106712863e-15\n",
      "1.6375067249647821e-15\n",
      "1.7215296503921503e-15\n",
      "8.730079999526987e-16\n",
      "7.44586753769494e-16\n",
      "1.0226929886767526e-15\n",
      "1.6912776597298674e-15\n",
      "1.2465265455864145e-15\n",
      "1.3655983301238745e-15\n",
      "1.8288036288786735e-15\n",
      "1.924971877291855e-15\n",
      "8.538866813553382e-16\n",
      "7.014880631748881e-16\n",
      "1.1527327400692458e-15\n",
      "1.1557035190199857e-15\n",
      "5.871851950584403e-16\n",
      "1.3264556227968707e-15\n",
      "1.1617451837048265e-15\n",
      "1.6647874421555108e-15\n",
      "1.539485479698381e-15\n",
      "1.203628306134788e-15\n",
      "2.053514607230778e-15\n",
      "1.4377216145875412e-15\n",
      "1.4721877164513224e-15\n",
      "1.2598912587947476e-16\n",
      "6.106007154667416e-16\n",
      "1.1178452319490853e-15\n",
      "4.3679215204335658e-16\n",
      "9.04341787711199e-16\n",
      "8.023741891559128e-16\n",
      "1.1638557544677231e-15\n",
      "1.8477715556389633e-15\n",
      "8.837539905440597e-16\n",
      "1.8255377641609787e-15\n",
      "8.364988504770728e-16\n",
      "7.089025040944729e-16\n",
      "9.288798140806685e-16\n",
      "6.7716497920241975e-16\n",
      "8.185109414436926e-16\n",
      "1.0351567139422644e-15\n",
      "1.2669323364312162e-15\n",
      "6.019810420624076e-16\n",
      "1.0792302921591937e-15\n",
      "6.118174563043645e-16\n",
      "1.2037704540061646e-15\n",
      "1.575250299538539e-15\n",
      "5.35788088185119e-16\n",
      "6.44694367605614e-16\n",
      "4.0436579922495517e-16\n",
      "9.207037160232037e-16\n",
      "1.8063090764268955e-16\n",
      "1.6033812808997785e-15\n",
      "4.534665726789944e-16\n",
      "1.1184152028582393e-15\n",
      "6.553187290665974e-16\n",
      "1.2472433269022885e-15\n",
      "8.826254463561585e-16\n",
      "5.366771123685974e-16\n",
      "7.355695504202316e-16\n",
      "1.1672873517026576e-15\n",
      "9.49770648356037e-16\n",
      "5.720226329999188e-16\n",
      "5.992254839687573e-16\n",
      "2.7051611644609294e-16\n",
      "6.208088175276094e-16\n",
      "2.2700770179750897e-15\n",
      "5.575936409148236e-16\n",
      "8.394245520850458e-16\n",
      "1.1904871320826723e-15\n",
      "7.89201219534168e-16\n",
      "8.749425633408415e-16\n",
      "1.2163422763408734e-15\n",
      "1.036744500259374e-15\n",
      "5.761910120102762e-16\n",
      "1.3240601994500417e-15\n",
      "1.6216776856411865e-15\n",
      "1.1538532065464155e-15\n",
      "2.5329574394235025e-15\n",
      "1.0717798085306334e-15\n",
      "1.0532845292582191e-15\n",
      "1.0207543906200783e-15\n",
      "8.444244911458793e-16\n",
      "4.938163897995721e-16\n",
      "1.0291633270916788e-15\n",
      "1.6259308861787402e-15\n",
      "6.766238540610141e-16\n",
      "9.144487380848813e-16\n",
      "2.0043451484784627e-16\n",
      "1.1541206568084486e-15\n",
      "1.7555184823151027e-15\n",
      "1.9703524319948866e-15\n",
      "8.222030036152673e-16\n",
      "8.516774141707564e-16\n",
      "5.440347562108436e-16\n",
      "1.097536596047613e-15\n",
      "1.0677377566790158e-15\n",
      "1.7386517944785803e-15\n",
      "1.7289789226364589e-15\n",
      "9.591821426373832e-16\n",
      "8.321370820782539e-16\n",
      "3.5974060086869786e-16\n",
      "7.324612069718211e-16\n",
      "1.4439055330721599e-15\n",
      "1.0084652056639494e-15\n",
      "8.304023592485198e-16\n",
      "9.723126288420292e-16\n",
      "4.549568678703906e-16\n",
      "4.590571334897353e-16\n",
      "1.3640146210641142e-15\n",
      "1.2222270553423762e-15\n",
      "7.793007708453687e-16\n",
      "6.440510110511732e-16\n",
      "1.3548873107121948e-15\n",
      "5.618051618146785e-16\n",
      "4.855719897382542e-16\n",
      "1.2582237921870016e-15\n",
      "3.373585032620504e-16\n",
      "1.5074841806089295e-15\n",
      "1.21366613105556e-15\n",
      "3.9622078506235395e-16\n",
      "1.0848034272400926e-15\n",
      "1.5858006144139512e-15\n",
      "1.1554952715906802e-15\n",
      "9.744408952956976e-16\n",
      "7.071870987403698e-16\n",
      "7.733015486520751e-16\n",
      "9.528662209688239e-16\n",
      "1.1625430541962026e-15\n",
      "1.0108935919808065e-15\n",
      "1.5544354181815499e-15\n",
      "1.0177561533602724e-15\n",
      "1.0620025187984704e-15\n",
      "6.484401202646436e-16\n",
      "1.4890887443771593e-15\n",
      "1.3048376731016095e-15\n",
      "8.767662692210582e-16\n",
      "9.224581090432184e-16\n",
      "9.503167233104763e-16\n",
      "1.4149780467780504e-15\n",
      "1.3365908644894056e-15\n",
      "1.1893680549968337e-15\n",
      "1.2085515332220257e-15\n",
      "1.4628398426675712e-15\n",
      "9.215111650213608e-16\n",
      "9.267223960962366e-16\n",
      "7.077018598070044e-16\n",
      "8.007961479354968e-16\n",
      "1.131787457513564e-15\n",
      "2.358673973319359e-15\n",
      "8.458997187562963e-16\n",
      "1.0543105713683134e-15\n",
      "1.4096518020805796e-15\n",
      "8.238389461467169e-16\n",
      "4.700288663173161e-16\n",
      "1.0355029640921254e-15\n",
      "8.634229599072544e-16\n",
      "4.08355828455617e-16\n",
      "3.131276760907468e-16\n",
      "1.447608865099863e-16\n",
      "1.0319844070373394e-15\n",
      "9.061965800589828e-16\n",
      "1.1510677233446204e-15\n",
      "1.0056785775957841e-15\n",
      "4.674528024582883e-16\n",
      "7.561108810709e-16\n",
      "1.0332073552119235e-15\n",
      "1.0787758058500636e-15\n",
      "8.165217358020616e-16\n",
      "1.0289150952091066e-15\n",
      "1.3254524919951458e-15\n",
      "7.301799720072611e-16\n",
      "1.1229758730936057e-15\n",
      "2.2901680987944634e-15\n",
      "1.0976712795093081e-15\n",
      "8.728317827393688e-16\n",
      "1.5686094404357318e-15\n",
      "1.4488452345806995e-15\n",
      "1.135226346083709e-15\n",
      "4.112220542491933e-16\n",
      "8.86503757957869e-16\n",
      "8.310738048808201e-16\n",
      "7.575800290750146e-16\n",
      "1.184202457854197e-15\n",
      "8.819709095436149e-16\n",
      "9.387145183609429e-16\n",
      "6.755793666080827e-16\n",
      "7.760407725587069e-16\n",
      "5.143712148068528e-16\n",
      "9.402897895322502e-16\n",
      "6.968186233864546e-16\n",
      "8.749884738805883e-16\n",
      "5.398456247506083e-16\n",
      "4.774401256271014e-16\n",
      "7.461198785172174e-16\n",
      "3.024743884213993e-15\n",
      "1.4185129520277471e-15\n",
      "1.6510429052057783e-15\n",
      "1.2522095019859208e-15\n",
      "7.468095141090255e-16\n",
      "1.1772613679936915e-15\n",
      "7.590170006773591e-16\n",
      "1.8386126677373324e-15\n",
      "5.624446153632172e-16\n",
      "7.324084756984459e-16\n",
      "8.098289494922827e-16\n",
      "5.963390972804026e-16\n",
      "5.762828261749357e-16\n",
      "7.868100173457891e-16\n",
      "1.2746183990139117e-15\n",
      "1.0438953658286241e-15\n",
      "1.1150771876095973e-15\n",
      "3.4486002019706913e-16\n",
      "1.8433375090215113e-15\n",
      "9.660633527255992e-16\n",
      "1.261048731030747e-15\n",
      "8.374855666755769e-16\n",
      "1.885628128520421e-15\n",
      "1.1423765415954409e-15\n",
      "8.100823386695647e-16\n",
      "8.891069575645268e-16\n",
      "5.974649260334099e-16\n",
      "6.613521545172719e-16\n",
      "1.1032162579450999e-15\n",
      "8.141023245533451e-16\n",
      "9.09136233943902e-16\n",
      "1.1816148307691954e-15\n",
      "1.493456739850059e-15\n",
      "9.776002433335736e-16\n",
      "2.305737115441925e-16\n",
      "1.5504326272675238e-15\n",
      "1.3023103687019244e-15\n",
      "1.2337321622763098e-15\n",
      "4.748675586330743e-16\n",
      "7.24220062703423e-16\n",
      "9.173402878983089e-16\n",
      "5.797110739937595e-16\n",
      "9.108078866089951e-16\n",
      "1.1286265260583362e-15\n",
      "1.1267371924680503e-15\n",
      "1.4143457053201483e-15\n",
      "6.137184374300885e-16\n",
      "1.535348305109601e-15\n",
      "7.353450183078339e-16\n",
      "4.0501714926764657e-16\n",
      "1.021413518091062e-15\n",
      "2.7456006010299285e-16\n",
      "1.3247996227743976e-15\n",
      "1.3043407295619894e-15\n",
      "6.113873361604208e-16\n",
      "5.762326887491113e-16\n",
      "1.2596903323068584e-15\n",
      "1.1284254685271757e-15\n",
      "1.5425087872065598e-15\n",
      "2.4464081894994817e-15\n",
      "1.2180124893276341e-15\n",
      "7.680722823416087e-16\n",
      "6.437038236686254e-16\n",
      "1.5293049497876656e-15\n",
      "7.463859828585655e-16\n",
      "1.144157939632146e-15\n",
      "9.823851517626739e-16\n",
      "8.888982489081773e-16\n",
      "1.2016079677475893e-15\n",
      "9.804985717604436e-16\n",
      "1.2539838941868219e-15\n",
      "8.954733804012217e-16\n",
      "1.4809344749892888e-15\n",
      "9.017352782869436e-16\n",
      "7.753119473108758e-16\n",
      "9.089563544425512e-16\n",
      "6.072809921194084e-16\n",
      "1.0109790797659709e-15\n",
      "7.394623252468346e-16\n",
      "1.1314917734069487e-15\n",
      "4.0251355272808095e-16\n",
      "1.7194479000195544e-15\n",
      "8.094792447991278e-16\n",
      "1.1919430779463628e-15\n",
      "1.891655017607445e-15\n",
      "1.380003227340509e-15\n",
      "8.933418619151982e-16\n",
      "7.24338130349463e-16\n",
      "9.990050955777366e-16\n",
      "6.522393664395886e-16\n",
      "1.0733561502126584e-15\n",
      "6.609339461153758e-16\n",
      "8.012061881903746e-16\n",
      "1.546555888111787e-15\n",
      "1.44212617869612e-15\n",
      "1.1956215917301017e-15\n",
      "1.5127753629282096e-15\n",
      "5.532590267096369e-16\n",
      "5.885359059156573e-16\n",
      "7.272734514721956e-16\n",
      "8.85339200343253e-16\n",
      "7.096589653012672e-16\n",
      "5.700293135597599e-16\n",
      "1.2839572029815842e-15\n",
      "1.095948086722347e-15\n",
      "1.2379442815140177e-15\n",
      "6.855813444337081e-16\n",
      "1.1880770724592978e-15\n",
      "1.3158935785598081e-15\n",
      "1.0809865521509914e-15\n",
      "9.937189879754935e-16\n",
      "1.37562606525586e-15\n",
      "4.858160732023346e-16\n",
      "1.2311540432560214e-15\n",
      "5.201964858177787e-16\n",
      "1.729619064388616e-15\n",
      "8.779565375462239e-16\n",
      "3.24985915934243e-16\n",
      "8.616725094219029e-16\n",
      "1.080788872957764e-15\n",
      "1.1781847361629582e-15\n",
      "1.6188864271781797e-15\n",
      "5.195304565714914e-16\n",
      "7.1798306857400545e-16\n",
      "1.3553206756257503e-15\n",
      "8.657255759601679e-16\n",
      "6.517809644247526e-16\n",
      "1.535426266613354e-15\n",
      "7.809763308689583e-16\n",
      "1.688842802156048e-15\n",
      "2.810732485178672e-16\n",
      "8.237393694491137e-16\n",
      "5.030163524594305e-16\n",
      "1.3693364433857947e-15\n",
      "8.398543649417952e-16\n",
      "1.181334384847743e-15\n",
      "1.1067337594556856e-15\n",
      "1.0286092576705688e-15\n",
      "7.949678853961785e-16\n",
      "9.526724625476325e-16\n",
      "1.4521093438024937e-15\n",
      "4.229894934735097e-16\n",
      "7.482626178024275e-16\n",
      "8.222595447264458e-16\n",
      "1.5492757845377028e-15\n",
      "1.769183849543781e-15\n",
      "7.740308975303577e-16\n",
      "9.619359856242143e-16\n",
      "1.2139815922067656e-15\n",
      "1.655295311577241e-16\n",
      "2.398492361628704e-15\n",
      "5.929861525129781e-16\n",
      "1.3977640952969769e-15\n",
      "1.1955774203524387e-15\n",
      "4.592559577798674e-16\n",
      "1.0064396922976048e-15\n",
      "8.909087718225613e-16\n",
      "1.1032795736232556e-15\n",
      "8.864040913581342e-16\n",
      "1.6900949800937675e-15\n",
      "2.613754206229171e-16\n",
      "1.9524469955359207e-15\n",
      "3.1886969385635354e-16\n",
      "1.8704133843438114e-15\n",
      "4.780738426487071e-16\n",
      "1.7914702884155128e-15\n",
      "2.4348274142623754e-15\n",
      "1.6591600428716086e-15\n",
      "1.1489393736411265e-15\n",
      "8.7651116721675575e-16\n",
      "1.5019560110274467e-15\n",
      "1.0376508794921099e-15\n",
      "6.11404843308522e-16\n",
      "5.26946916473677e-16\n",
      "1.256419851398552e-15\n",
      "1.7459408309263104e-15\n",
      "1.1526695630532691e-15\n",
      "1.1075432232923245e-15\n",
      "1.1704172590610211e-15\n",
      "1.230779583028714e-15\n",
      "1.2187099499129258e-15\n",
      "8.230701797775044e-16\n",
      "8.927170599494356e-16\n",
      "1.3217189329690885e-15\n",
      "9.051425223948994e-16\n",
      "2.0716564752881674e-15\n",
      "1.0257255168108776e-15\n",
      "1.2032293907853183e-15\n",
      "9.802301506457484e-16\n",
      "4.487151964511666e-16\n",
      "8.155529516483433e-16\n"
     ]
    }
   ],
   "source": [
    "ft1 =  np.stack(FR_dic.values(), axis=2)\n",
    "ft2 = np.stack(TI_dic.values(), axis=2)\n",
    "# feature_tensor=ft2\n",
    "feature_tensor = np.dstack((ft1, ft2))\n",
    "\n",
    "total_pc = np.zeros(shape=[2,n_stocks,n_features])\n",
    "list_of_features = [] #Split the features into different look_back_duration time slots. \n",
    "\n",
    "for k in range(0,len(feature_tensor),look_back_duration): \n",
    "    core, factors = tucker(feature_tensor[k:k+look_back_duration], rank= [2,n_stocks,n_features])\n",
    "    pc = tl.tenalg.mode_dot(feature_tensor[k:k+look_back_duration], factors[1].T, mode = 1)\n",
    "    total_pc = np.vstack((total_pc,pc))\n",
    "    rec = tl.tucker_to_tensor((core,factors))\n",
    "    rec_error = tl.norm(rec - feature_tensor[k:k+look_back_duration])/tl.norm(feature_tensor[k:k+look_back_duration])\n",
    "    print(rec_error)\n",
    "total_pc = total_pc[2:]\n",
    "list_of_features = [] #Split the features into different look_back_duration time slots. \n",
    "for i in range(0,len(total_pc)-look_back_duration-look_forward+1) : \n",
    "    list_of_features.append(total_pc[i:i+look_back_duration, :,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(7) : \n",
    "#     plt.figure()\n",
    "#     for n in range(n_stocks) :\n",
    "#         plt.plot(total_pc[:,n,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solo_features(solo_df, lb_duration, lf_duration) :\n",
    "    list_of_solo_f = []\n",
    "    #SOLO FEATURES\n",
    "    for i in range(0,len(solo_df)-lb_duration-lf_duration+1) : \n",
    "        list_of_solo_f.append(solo_df[i:i+lb_duration].to_numpy())\n",
    "    return list_of_solo_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET THE RETURNS OF COMPANY X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the training data of test company \n",
    "solo_df = pd.DataFrame()\n",
    "\n",
    "test_company = 'AAPL'\n",
    "k = 0\n",
    "for key,value in FR_dic.items() : \n",
    "    if k == 0 : \n",
    "        solo_df[key] = value[f'{test_company} US Equity']\n",
    "    if k > 0 : \n",
    "        solo_df[key] = value[f'{test_company} US Equity'+ f'.{k}']\n",
    "    k+=1\n",
    "\n",
    "for key,value in TI_dic.items() : \n",
    "    solo_df[key] = value[f'{test_company} US Equity']\n",
    "    \n",
    "\n",
    "for col in FR_dic['PX_LAST'] : \n",
    "    if col.startswith(test_company) : \n",
    "        y_predict = FR_dic['PX_LAST'][col]\n",
    "\n",
    "test_comp_returns = pd.DataFrame(y_predict).set_axis(['Log Returns'], axis = 1)\n",
    "test_comp_returns['Cumulative Log Returns'] = test_comp_returns['Log Returns'].cumsum()\n",
    "def get_n_week_retuns(log_returns, look_back_duration, lookforward) : \n",
    "    sdate = test_comp_returns.index.values[0]\n",
    "    edate = test_comp_returns.index.values[-1]\n",
    "    s = (log_returns\n",
    "     .reset_index()\n",
    "     .iloc[look_back_duration:]\n",
    "    )\n",
    "    n_week_retuns = s.rolling(lookforward).sum()\n",
    "    n_week_retuns.index = list(pd.date_range(sdate ,edate + pd.to_timedelta(2, unit='D') ,freq='w') - pd.to_timedelta(2, unit='D'))[:-look_back_duration]\n",
    "    return n_week_retuns.dropna() \n",
    "\n",
    "\n",
    "def to_simple_return(cum_log_ret) : \n",
    "    return np.exp(cum_log_ret) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n",
      "(2, 12, 6)\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "# x_ret_list contains the \n",
    "y_arr =  get_n_week_retuns(test_comp_returns['Log Returns'], look_back_duration = look_back_duration, lookforward = look_forward)\n",
    "\n",
    "y_arr[(y_arr > 0)] = 1\n",
    "\n",
    "y_arr[y_arr <= 0] = -1\n",
    "\n",
    "y_ret = y_arr.to_numpy().flatten()\n",
    "\n",
    "\n",
    "list_of_solo_f = get_solo_features(solo_df, look_back_duration, look_forward)\n",
    "list_of_combined_features = []\n",
    "\n",
    "\n",
    "for feature_t,solo_feature in zip(list_of_features,list_of_solo_f)  :\n",
    "    solo_feature = solo_feature[:,newaxis,:]\n",
    "    print(feature_t.shape)\n",
    "\n",
    "    tmp = np.concatenate((feature_t,solo_feature), axis=1)\n",
    "    list_of_combined_features.append(tmp)\n",
    "\n",
    "\n",
    "X_Cols = [tensor.flatten() for tensor in list_of_features ]\n",
    "\n",
    "S_Cols = [mat.flatten() for mat in list_of_solo_f]\n",
    "\n",
    "C_Cols = [tensor.flatten() for tensor in list_of_combined_features]\n",
    "\n",
    "C_val, Y_val = C_Cols[-120:], y_ret[-120:]\n",
    "\n",
    "S_val = S_Cols[-120:]\n",
    "\n",
    "X_Cols, S_Cols, C_Cols = X_Cols[:-120], S_Cols[:-120] , C_Cols[:-120]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from numpy import newaxis\n",
    " \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_Cols, y_ret[:-120], random_state = 42, shuffle=False)\n",
    "\n",
    "S_train, S_test, Y_train, Y_test = train_test_split(S_Cols, y_ret[:-120], random_state = 42, shuffle=False)\n",
    "\n",
    "C_train, C_test, Y_train, Y_test = train_test_split(C_Cols, y_ret[:-120], random_state = 42, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, mean_squared_error, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zackx\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=2, estimator=RandomForestClassifier(), n_iter=1, n_jobs=4,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [False, True],\n",
       "                                        &#x27;max_depth&#x27;: [3],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [2],\n",
       "                                        &#x27;min_samples_split&#x27;: [2],\n",
       "                                        &#x27;n_estimators&#x27;: [1000, 2000]},\n",
       "                   random_state=144, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=2, estimator=RandomForestClassifier(), n_iter=1, n_jobs=4,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [False, True],\n",
       "                                        &#x27;max_depth&#x27;: [3],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [2],\n",
       "                                        &#x27;min_samples_split&#x27;: [2],\n",
       "                                        &#x27;n_estimators&#x27;: [1000, 2000]},\n",
       "                   random_state=144, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=2, estimator=RandomForestClassifier(), n_iter=1, n_jobs=4,\n",
       "                   param_distributions={'bootstrap': [False, True],\n",
       "                                        'max_depth': [3],\n",
       "                                        'max_features': ['auto'],\n",
       "                                        'min_samples_leaf': [2],\n",
       "                                        'min_samples_split': [2],\n",
       "                                        'n_estimators': [1000, 2000]},\n",
       "                   random_state=144, verbose=4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## HYPERPARAMETER TUNING\n",
    "# n_estimators = [1000,2000]\n",
    "\n",
    "# max_features = ['auto']\n",
    "\n",
    "# max_depth = [4]\n",
    "\n",
    "# min_samples_split = [2]\n",
    "\n",
    "# min_samples_leaf = [2]\n",
    "\n",
    "# bootstrap = [False, True]\n",
    "\n",
    "\n",
    "# random_grid = {\n",
    "    \n",
    "# 'n_estimators': n_estimators,\n",
    "\n",
    "# 'max_features': max_features,\n",
    "\n",
    "# 'max_depth': max_depth,\n",
    "\n",
    "# 'min_samples_split': min_samples_split,\n",
    "\n",
    "# 'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "# 'bootstrap' : bootstrap\n",
    "\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Using 5 fold cross validation\n",
    "# # Search across 100 different combintations and use all available cores\n",
    "\n",
    "# rf_random = RandomizedSearchCV(\n",
    "#     estimator = rf,\n",
    "#     param_distributions= random_grid,\n",
    "#     n_iter = 1, \n",
    "#     verbose= 4,\n",
    "#     cv = 2,\n",
    "#     random_state= 144,\n",
    "#     n_jobs = 4,\n",
    "    \n",
    "# )\n",
    "\n",
    "# rf_random.fit(C_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number of ones : 69\n",
      "Predicted number of ones : 107\n",
      "Baseline if all were ones : 0.575\n",
      "Actual number of -1 : 51\n",
      "Predicted number of -1 : 13\n",
      "Baseline if all were -1 : 0.425\n"
     ]
    }
   ],
   "source": [
    "def baseline(Y_test, prediction) : \n",
    "    Y_test = Y_test.flatten()\n",
    "\n",
    "    all_ones = np.ones(len(Y_test))\n",
    "    all_neg_ones = np.ones(len(Y_test))* -1\n",
    "    print(f\"Actual number of ones : {np.count_nonzero(Y_test == 1)}\")\n",
    "    print(f\"Predicted number of ones : {np.count_nonzero(prediction == 1)}\")\n",
    "    print(f\"Baseline if all were ones : {np.count_nonzero((all_ones==Y_test)) / len(Y_test)}\")\n",
    "\n",
    "    print(f\"Actual number of -1 : {np.count_nonzero(Y_test == -1)}\")\n",
    "    print(f\"Predicted number of -1 : {np.count_nonzero(prediction == -1)}\")\n",
    "    print(f\"Baseline if all were -1 : {np.count_nonzero((all_neg_ones==Y_test)) / len(Y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zackx\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6724806201550387\n",
      "[[ 56 165]\n",
      " [  4 291]]\n",
      "0.5872093023255814\n",
      "[[15 48]\n",
      " [23 86]]\n",
      "0.5083333333333333\n",
      "[[24 27]\n",
      " [32 37]]\n",
      "Actual number of ones : 109\n",
      "Predicted number of ones : 134\n",
      "Baseline if all were ones : 0.6337209302325582\n",
      "Actual number of -1 : 63\n",
      "Predicted number of -1 : 38\n",
      "Baseline if all were -1 : 0.36627906976744184\n"
     ]
    }
   ],
   "source": [
    "rand_frst_S = RandomForestClassifier(n_estimators= 500, min_samples_split = 2, min_samples_leaf=3,  max_features='auto', max_depth = 3, bootstrap = True, random_state=22)\n",
    "\n",
    "rand_frst_S.fit(S_train,Y_train)\n",
    "\n",
    "\n",
    "prediction = rand_frst_S.predict(S_train)\n",
    "print(accuracy_score(Y_train, prediction))\n",
    "print(confusion_matrix(Y_train, prediction))\n",
    "\n",
    "prediction = rand_frst_S.predict(S_test)\n",
    "print(accuracy_score(Y_test, prediction))\n",
    "print(confusion_matrix(Y_test, prediction))\n",
    "\n",
    "prediction2 = rand_frst_S.predict(S_val)\n",
    "print(accuracy_score(Y_val, prediction2))\n",
    "print(confusion_matrix(Y_val, prediction2))\n",
    "\n",
    "baseline(Y_test,prediction)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random grid:  {'n_estimators': [1000, 2000], 'max_features': ['auto'], 'max_depth': [3], 'min_samples_split': [2], 'min_samples_leaf': [2], 'bootstrap': [False, True]} \n",
      "\n",
      "Best Parameters:  {'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 3, 'bootstrap': False}  \n",
      "\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 3, 'bootstrap': False}\n",
      "0.7189922480620154\n",
      "[[ 76 145]\n",
      " [  0 295]]\n",
      "0.6046511627906976\n",
      "[[ 5 58]\n",
      " [10 99]]\n",
      "0.5333333333333333\n",
      "[[14 37]\n",
      " [19 50]]\n"
     ]
    }
   ],
   "source": [
    "# print('Random grid: ', random_grid, '\\n')\n",
    "\n",
    "# print('Best Parameters: ', rf_random.best_params_, ' \\n')\n",
    "\n",
    "# # print(rf_random.best_score_)\n",
    "# print(rf_random.best_params_)\n",
    "# target_names = ['Red', 'Green']\n",
    "# prediction = rf_random.predict(C_train)\n",
    "# print(accuracy_score(Y_train, prediction))\n",
    "# # print(classification_report(Y_train, prediction, target_names=target_names))\n",
    "# print(confusion_matrix(Y_train, prediction))\n",
    "\n",
    "\n",
    "# prediction = rf_random.predict(C_test)\n",
    "# print(accuracy_score(Y_test, prediction))\n",
    "# # print(classification_report(Y_test, prediction, target_names=target_names))\n",
    "# print(confusion_matrix(Y_test, prediction))\n",
    "\n",
    "\n",
    "# prediction_val1= rf_random.predict(C_val)\n",
    "# print(accuracy_score(Y_val, prediction_val1))\n",
    "# print(confusion_matrix(Y_val, prediction_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.PB.3          0.001304\n",
      "2.PE.12         0.001553\n",
      "1.PB.10         0.001912\n",
      "2.PE.4          0.002009\n",
      "1.PS.11         0.002021\n",
      "1.PS.6          0.002119\n",
      "2.PB.6          0.002441\n",
      "1.PX_LAST.6     0.002627\n",
      "1.MOM.3         0.002654\n",
      "2.PE.11         0.002725\n",
      "2.HURST.6       0.002760\n",
      "1.MOM.5         0.002814\n",
      "1.PS.5          0.002876\n",
      "2.PX_LAST.3     0.002897\n",
      "1.PS.10         0.002973\n",
      "2.PE.8          0.003006\n",
      "2.PX_LAST.12    0.003062\n",
      "1.PX_LAST.11    0.003073\n",
      "1.PB.5          0.003086\n",
      "2.MOM.7         0.003110\n",
      "1.HURST.11      0.003127\n",
      "2.PX_LAST.6     0.003133\n",
      "2.HURST.0       0.003172\n",
      "2.PB.12         0.003177\n",
      "1.PX_LAST.7     0.003188\n",
      "2.PB.4          0.003190\n",
      "1.HURST.6       0.003288\n",
      "1.PE.9          0.003340\n",
      "2.PS.10         0.003350\n",
      "1.PB.9          0.003424\n",
      "1.MOM.9         0.003444\n",
      "2.PB.1          0.003468\n",
      "2.MOM.9         0.003676\n",
      "2.PS.6          0.003703\n",
      "2.PS.12         0.003720\n",
      "1.PE.10         0.003753\n",
      "2.PS.4          0.003774\n",
      "2.MOM.2         0.003821\n",
      "2.HURST.11      0.003843\n",
      "1.HURST.5       0.003881\n",
      "1.PB.0          0.003916\n",
      "2.MOM.3         0.003950\n",
      "2.PB.11         0.003973\n",
      "1.PX_LAST.0     0.003978\n",
      "1.PX_LAST.2     0.004053\n",
      "2.PX_LAST.1     0.004070\n",
      "1.PS.0          0.004071\n",
      "2.HURST.8       0.004208\n",
      "1.PE.5          0.004218\n",
      "2.PB.10         0.004265\n",
      "2.PX_LAST.11    0.004423\n",
      "2.HURST.5       0.004429\n",
      "2.PB.9          0.004432\n",
      "1.PX_LAST.5     0.004455\n",
      "1.PX_LAST.8     0.004493\n",
      "1.HURST.12      0.004495\n",
      "2.PX_LAST.0     0.004540\n",
      "1.PX_LAST.10    0.004571\n",
      "2.PX_LAST.2     0.004618\n",
      "1.PE.7          0.004743\n",
      "2.PS.1          0.004753\n",
      "2.PE.10         0.004800\n",
      "1.MOM.8         0.004827\n",
      "2.PE.3          0.004842\n",
      "1.HURST.7       0.004913\n",
      "2.PE.1          0.004915\n",
      "2.PB.0          0.004989\n",
      "2.PS.9          0.005032\n",
      "1.PE.2          0.005036\n",
      "2.PX_LAST.7     0.005043\n",
      "1.PS.4          0.005081\n",
      "1.HURST.4       0.005087\n",
      "1.PX_LAST.9     0.005161\n",
      "2.MOM.4         0.005174\n",
      "2.PE.6          0.005265\n",
      "2.MOM.10        0.005392\n",
      "2.MOM.5         0.005397\n",
      "1.HURST.8       0.005429\n",
      "1.PX_LAST.4     0.005448\n",
      "1.HURST.0       0.005569\n",
      "1.HURST.2       0.005650\n",
      "2.HURST.10      0.005737\n",
      "1.PE.4          0.005748\n",
      "1.PE.8          0.005766\n",
      "2.MOM.1         0.005916\n",
      "1.PB.11         0.005993\n",
      "2.HURST.2       0.006024\n",
      "2.PX_LAST.10    0.006049\n",
      "2.MOM.11        0.006123\n",
      "2.PS.0          0.006191\n",
      "2.PB.8          0.006256\n",
      "1.MOM.7         0.006326\n",
      "2.MOM.0         0.006387\n",
      "1.PE.12         0.006390\n",
      "2.PS.3          0.006394\n",
      "1.PE.11         0.006406\n",
      "2.PE.7          0.006505\n",
      "1.PX_LAST.12    0.006682\n",
      "2.PX_LAST.4     0.006693\n",
      "1.PE.6          0.006706\n",
      "1.PS.2          0.006775\n",
      "2.HURST.4       0.006794\n",
      "1.PS.7          0.006805\n",
      "2.PS.2          0.006834\n",
      "1.PS.9          0.006888\n",
      "2.HURST.1       0.006927\n",
      "1.HURST.1       0.006980\n",
      "1.MOM.11        0.007004\n",
      "2.PE.9          0.007092\n",
      "2.MOM.6         0.007217\n",
      "1.PS.8          0.007269\n",
      "1.MOM.10        0.007338\n",
      "2.PB.2          0.007372\n",
      "1.HURST.9       0.007451\n",
      "2.HURST.9       0.007471\n",
      "2.PX_LAST.9     0.007547\n",
      "2.PX_LAST.5     0.007697\n",
      "2.PE.2          0.007933\n",
      "2.PS.7          0.008030\n",
      "1.PB.7          0.008172\n",
      "1.PS.12         0.008219\n",
      "1.MOM.6         0.008517\n",
      "1.PB.4          0.008782\n",
      "2.MOM.8         0.008862\n",
      "2.MOM.12        0.009007\n",
      "2.PS.8          0.009082\n",
      "2.PS.5          0.009253\n",
      "1.HURST.10      0.009395\n",
      "2.PS.11         0.009441\n",
      "1.HURST.3       0.009496\n",
      "1.PX_LAST.1     0.009730\n",
      "1.PB.3          0.009932\n",
      "2.HURST.7       0.009980\n",
      "1.PB.6          0.010044\n",
      "1.MOM.4         0.010052\n",
      "1.PB.8          0.010097\n",
      "2.PX_LAST.8     0.010151\n",
      "1.MOM.12        0.010166\n",
      "1.PX_LAST.3     0.010766\n",
      "2.HURST.3       0.011340\n",
      "1.PB.12         0.011449\n",
      "1.PS.3          0.011480\n",
      "2.PB.7          0.011905\n",
      "1.PE.0          0.012019\n",
      "2.PB.5          0.012227\n",
      "1.PB.2          0.012359\n",
      "1.PE.1          0.013241\n",
      "1.PB.1          0.013454\n",
      "1.MOM.2         0.013566\n",
      "1.MOM.0         0.013647\n",
      "1.PE.3          0.013717\n",
      "2.HURST.12      0.014632\n",
      "1.PS.1          0.015239\n",
      "2.PE.0          0.016934\n",
      "2.PE.5          0.019952\n",
      "1.MOM.1         0.023781\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# feature_imp = pd.Series(rf_random.best_estimator_.feature_importances_)\n",
    "feature_imp = pd.Series(rand_frst_C.feature_importances_)\n",
    "\n",
    "feature_imp = feature_imp.sort_values(ascending=True)\n",
    "srted_indx_list = feature_imp.index.to_list()\n",
    "\n",
    "col_names= ['PE', 'PX_LAST', 'PS', 'PB', 'HURST', 'MOM']\n",
    "# col_names = ['BB_PERCENT', 'HURST', 'MOM', 'ROC','RSI','WLPR']\n",
    "\n",
    "def get_feature_name (srted_indx_list) :\n",
    "    name_list = []\n",
    "    for num in srted_indx_list : \n",
    "        half = len(srted_indx_list)/2 \n",
    "        if num < half : \n",
    "            rows = num // n_features\n",
    "            cols = num % n_features\n",
    "\n",
    "            name = f'1.{col_names[cols]}.{rows}' \n",
    "            \n",
    "        else : \n",
    "            rows = num // n_features - n_stocks - 1\n",
    "            cols = num % n_features\n",
    "            name = f'2.{col_names[cols]}.{rows}' \n",
    "    \n",
    "        name_list.append(name)\n",
    "    \n",
    "    return name_list\n",
    "\n",
    "\n",
    "feature_imp.index = (get_feature_name(srted_indx_list) )\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(feature_imp)\n",
    "\n",
    "\n",
    "        #second time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.BB_PERCENT.5     0.047663\n",
      "1.WLPR.1           0.037198\n",
      "1.WLPR.10          0.036964\n",
      "1.MOM.1            0.033582\n",
      "1.BB_PERCENT.3     0.029337\n",
      "1.ROC.1            0.028627\n",
      "2.MOM.11           0.025825\n",
      "1.ROC.2            0.025127\n",
      "1.BB_PERCENT.1     0.024101\n",
      "1.ROC.8            0.021638\n",
      "1.WLPR.2           0.020580\n",
      "2.BB_PERCENT.0     0.018286\n",
      "1.RSI.9            0.017695\n",
      "1.MOM.12           0.016158\n",
      "1.HURST.3          0.015845\n",
      "1.BB_PERCENT.0     0.014275\n",
      "1.WLPR.0           0.013896\n",
      "2.RSI.9            0.012093\n",
      "1.ROC.3            0.011656\n",
      "2.ROC.5            0.011215\n",
      "2.HURST.8          0.011055\n",
      "1.MOM.3            0.010858\n",
      "1.WLPR.4           0.010455\n",
      "1.RSI.1            0.010283\n",
      "2.BB_PERCENT.7     0.010170\n",
      "2.RSI.12           0.010145\n",
      "2.RSI.3            0.009812\n",
      "1.WLPR.6           0.009641\n",
      "1.RSI.10           0.009613\n",
      "2.BB_PERCENT.2     0.009348\n",
      "2.ROC.0            0.008905\n",
      "1.ROC.6            0.008773\n",
      "2.WLPR.12          0.008502\n",
      "2.ROC.2            0.008192\n",
      "1.MOM.8            0.008186\n",
      "1.WLPR.12          0.008094\n",
      "2.RSI.7            0.007580\n",
      "2.WLPR.4           0.007005\n",
      "2.HURST.9          0.006661\n",
      "2.MOM.5            0.006320\n",
      "2.WLPR.11          0.006080\n",
      "2.RSI.4            0.006076\n",
      "2.MOM.0            0.006041\n",
      "1.HURST.9          0.006019\n",
      "2.RSI.1            0.005909\n",
      "2.MOM.4            0.005884\n",
      "1.HURST.1          0.005639\n",
      "1.RSI.2            0.005484\n",
      "2.MOM.8            0.005462\n",
      "2.HURST.0          0.005347\n",
      "2.BB_PERCENT.9     0.005247\n",
      "1.BB_PERCENT.8     0.004949\n",
      "2.MOM.7            0.004919\n",
      "1.RSI.7            0.004824\n",
      "1.BB_PERCENT.11    0.004788\n",
      "1.BB_PERCENT.7     0.004747\n",
      "2.HURST.5          0.004743\n",
      "1.ROC.4            0.004697\n",
      "1.HURST.5          0.004672\n",
      "2.ROC.11           0.004636\n",
      "1.ROC.12           0.004480\n",
      "2.BB_PERCENT.6     0.004426\n",
      "1.RSI.3            0.004365\n",
      "2.RSI.2            0.004348\n",
      "1.BB_PERCENT.2     0.004329\n",
      "1.HURST.12         0.004328\n",
      "2.MOM.1            0.004284\n",
      "1.WLPR.11          0.004271\n",
      "1.MOM.9            0.004071\n",
      "2.ROC.7            0.003950\n",
      "2.MOM.2            0.003919\n",
      "1.HURST.4          0.003830\n",
      "2.HURST.3          0.003695\n",
      "1.WLPR.8           0.003682\n",
      "2.HURST.2          0.003641\n",
      "2.WLPR.5           0.003626\n",
      "1.RSI.11           0.003611\n",
      "2.MOM.9            0.003611\n",
      "1.ROC.7            0.003515\n",
      "1.ROC.5            0.003468\n",
      "2.ROC.6            0.003408\n",
      "1.MOM.7            0.003342\n",
      "1.RSI.0            0.003296\n",
      "2.HURST.11         0.003247\n",
      "1.ROC.11           0.003245\n",
      "1.BB_PERCENT.10    0.003230\n",
      "1.HURST.6          0.003215\n",
      "2.WLPR.1           0.003214\n",
      "1.BB_PERCENT.4     0.003190\n",
      "2.HURST.10         0.003130\n",
      "1.MOM.4            0.003084\n",
      "2.HURST.4          0.003055\n",
      "2.ROC.8            0.003043\n",
      "2.WLPR.0           0.003040\n",
      "1.BB_PERCENT.9     0.003029\n",
      "2.WLPR.9           0.002952\n",
      "2.WLPR.6           0.002934\n",
      "1.ROC.9            0.002891\n",
      "2.RSI.5            0.002862\n",
      "1.WLPR.7           0.002830\n",
      "2.RSI.6            0.002793\n",
      "2.ROC.4            0.002760\n",
      "1.MOM.2            0.002731\n",
      "2.RSI.0            0.002729\n",
      "2.HURST.7          0.002696\n",
      "2.BB_PERCENT.8     0.002693\n",
      "1.BB_PERCENT.12    0.002666\n",
      "2.WLPR.10          0.002665\n",
      "1.RSI.8            0.002588\n",
      "2.MOM.12           0.002541\n",
      "2.ROC.10           0.002532\n",
      "2.ROC.1            0.002512\n",
      "2.ROC.12           0.002442\n",
      "2.BB_PERCENT.4     0.002422\n",
      "2.HURST.6          0.002420\n",
      "2.WLPR.2           0.002358\n",
      "2.RSI.11           0.002353\n",
      "1.WLPR.5           0.002328\n",
      "1.WLPR.3           0.002299\n",
      "1.MOM.6            0.002280\n",
      "2.ROC.3            0.002270\n",
      "2.MOM.10           0.002170\n",
      "2.BB_PERCENT.3     0.002166\n",
      "2.MOM.3            0.002158\n",
      "1.WLPR.9           0.002089\n",
      "1.RSI.12           0.002061\n",
      "1.HURST.10         0.002044\n",
      "1.BB_PERCENT.5     0.002003\n",
      "1.BB_PERCENT.6     0.001984\n",
      "1.HURST.11         0.001972\n",
      "2.WLPR.7           0.001922\n",
      "1.RSI.6            0.001905\n",
      "2.BB_PERCENT.12    0.001894\n",
      "2.BB_PERCENT.1     0.001791\n",
      "2.ROC.9            0.001767\n",
      "1.HURST.8          0.001766\n",
      "2.WLPR.3           0.001760\n",
      "1.MOM.0            0.001669\n",
      "2.MOM.6            0.001661\n",
      "1.MOM.11           0.001646\n",
      "2.HURST.1          0.001629\n",
      "2.WLPR.8           0.001622\n",
      "2.BB_PERCENT.11    0.001578\n",
      "2.RSI.10           0.001534\n",
      "1.RSI.5            0.001518\n",
      "1.HURST.2          0.001493\n",
      "2.HURST.12         0.001464\n",
      "2.RSI.8            0.001455\n",
      "1.HURST.7          0.001407\n",
      "1.ROC.10           0.001343\n",
      "1.MOM.5            0.001265\n",
      "1.ROC.0            0.001208\n",
      "1.MOM.10           0.001082\n",
      "2.BB_PERCENT.10    0.001026\n",
      "1.HURST.0          0.000994\n",
      "1.RSI.4            0.000667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(feature_imp.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_530316/777554630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Baseline if all were -1 : {np.count_nonzero((all_neg_ones==Y_test)) / len(Y_test)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mbaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# baseline(Y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_530316/777554630.py\u001b[0m in \u001b[0;36mbaseline\u001b[1;34m(Y_test, prediction)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mall_ones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# baseline(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FR_dic['PE']['AAPL US Equity'].rolling(50).mean().plot()\n",
    "# # FR_dic['PE']['SLB US Equity'].rolling(50).mean().plot()\n",
    "# aapl_acf = acf(FR_dic['PE']['AAPL US Equity'])\n",
    "# aapl_acf_2 = acf(FR_dic['PX_LAST']['AAPL US Equity.1'])\n",
    "# aapl_acf_3 = acf(FR_dic['PS']['AAPL US Equity.2'])\n",
    "# aapl_acf_4 = acf(FR_dic['PB']['AAPL US Equity.3'])\n",
    "# aapl_acf_5 = acf(TI_dic['HURST']['AAPL US Equity'])\n",
    "# aapl_acf_6 = acf(TI_dic['HURST']['AAPL US Equity'])\n",
    "\n",
    "# # aapl_acf_6 = acf(FR_dic['PB']['AAPL US Equity.3'])\n",
    "\n",
    "\n",
    "# test_df = pd.DataFrame([aapl_acf, aapl_acf_2, aapl_acf_3, aapl_acf_4, aapl_acf_5, aapl_acf_6]).T\n",
    "# test_df.columns = ['ACF of first-order difference of PE of AAPL', 'ACF of first-order difference of PX_LAST of AAPL', \n",
    "#                    'ACF of first-order difference of PS of AAPL', 'ACF of first-order difference of PB of AAPL', 'ACF of first-order difference of HURST of AAPL',\n",
    "#                    'ACF of first-order difference of Momentum of AAPL'\n",
    "#                   ] \n",
    "# test_df.index += 1\n",
    "# test_df.plot(kind='bar', figsize=(10,7))\n",
    "\n",
    "# plt.savefig('ACF.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e2c2ee5cf96267dcc7744f326d3a9b930733ff27582ed4f98f71c09c9039794"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
